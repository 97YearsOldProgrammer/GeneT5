#!/usr/bin/env python3

import argparse
import json
import random
import time
import pathlib
import csv

import torch
import torch.distributed as dist
import torch.utils.data  as data_utils
import transformers       as tf

from contextlib import nullcontext
from functools  import partial

import lib.tokenizer.hf as tk
import lib.model.seq2seq as mdl
import lib.util         as util
import lib.train        as train_util
import lib.grpo.algo    as grpo
import lib.grpo.reward  as reward


DEFAULTS = {
    'batch_size':     8,
    'group_size':     8,
    'lr':             1e-6,
    'beta':           0.05,
    'weight_decay':   0.01,
    'warmup_ratio':   0.05,
    'grad_accum':     4,
    'max_grad_norm':  1.0,
    'max_gen_length': 256,
    'temperature':    0.8,
    'top_k':          50,
    'top_p':          0.9,
    'exon_weight':    0.6,
    'gene_weight':    0.4,
}


GRPO_LOG_FIELDS = [
    'timestamp', 'elapsed_sec', 'step', 'total_steps',
    'reward_mean', 'reward_std', 'policy_loss', 'kl_loss', 'kl_mean',
    'moe_loss', 'total_loss', 'lr', 'gen_sec', 'update_sec',
]


class GRPOLogger:
    """CSV logger for GRPO training metrics"""

    def __init__(self, output_dir):

        self.log_path = pathlib.Path(output_dir) / "grpo_log.csv"
        self._file    = open(self.log_path, 'w', newline='')
        self._writer  = csv.DictWriter(self._file, fieldnames=GRPO_LOG_FIELDS)
        self._writer.writeheader()
        self._file.flush()
        self.start_time = time.time()

    def log(self, step, total_steps, reward_mean, reward_std,
            policy_loss, kl_loss, kl_mean, moe_loss, total_loss,
            lr, gen_sec, update_sec):

        row = {
            'timestamp':   f'{time.time():.0f}',
            'elapsed_sec': f'{time.time() - self.start_time:.1f}',
            'step':        step,
            'total_steps': total_steps,
            'reward_mean': f'{reward_mean:.4f}',
            'reward_std':  f'{reward_std:.4f}',
            'policy_loss': f'{policy_loss:.6f}',
            'kl_loss':     f'{kl_loss:.6f}',
            'kl_mean':     f'{kl_mean:.4f}',
            'moe_loss':    f'{moe_loss:.6f}',
            'total_loss':  f'{total_loss:.6f}',
            'lr':          f'{lr:.2e}',
            'gen_sec':     f'{gen_sec:.1f}',
            'update_sec':  f'{update_sec:.1f}',
        }
        self._writer.writerow(row)
        self._file.flush()

    def close(self):

        if self._file and not self._file.closed:
            self._file.close()


def generate_group(model, prefix_ids, group_size, tokenizer, args):
    """Generate G outputs per prompt from prefix_ids"""

    batch_size = prefix_ids.size(0)

    # Expand prefix_ids: [B, L] -> [B*G, L]
    expanded = prefix_ids.unsqueeze(1).expand(
        batch_size, group_size, -1
    ).reshape(batch_size * group_size, -1)

    generated = model.generate(
        prefix_ids   = expanded,
        max_length   = args.max_gen_length,
        temperature  = args.temperature,
        top_k        = args.top_k,
        top_p        = args.top_p,
        eos_token_id = tokenizer.eos_token_id,
        pad_token_id = tokenizer.pad_token_id,
    )

    return generated, expanded


def score_generations(generated, prefix_len, ref_features_list, sequences,
                      group_size, tokenizer, args):
    """Score generated sequences against references"""

    batch_size = len(ref_features_list)
    rewards    = []

    for i in range(batch_size):
        for g in range(group_size):
            idx        = i * group_size + g
            output_ids = generated[idx, prefix_len:].tolist()
            pred_text  = tokenizer.decode(output_ids)

            r = reward.composite_reward(
                pred_text, ref_features_list[i], sequences[i],
                exon_weight=args.exon_weight, gene_weight=args.gene_weight
            )
            rewards.append(r)

    return torch.tensor(rewards, dtype=torch.float32)


def main():

    parser = argparse.ArgumentParser(description='GRPO training for GeneT5')
    parser.add_argument('data', type=str, metavar='<data>',
        help='GRPO training data JSON (from prep_grpo)')
    parser.add_argument('output_dir', type=str, metavar='<output>',
        help='output directory')
    parser.add_argument('model_path', type=str, metavar='<model>',
        help='SFT model checkpoint directory')
    parser.add_argument('--batch_size', type=int, default=DEFAULTS['batch_size'],
        metavar='<int>', help='prompts per device per step [%(default)i]')
    parser.add_argument('--group_size', type=int, default=DEFAULTS['group_size'],
        metavar='<int>', help='generations per prompt [%(default)i]')
    parser.add_argument('--lr', type=float, default=DEFAULTS['lr'],
        metavar='<float>', help='learning rate [%(default)g]')
    parser.add_argument('--beta', type=float, default=DEFAULTS['beta'],
        metavar='<float>', help='KL penalty coefficient [%(default)g]')
    parser.add_argument('--weight_decay', type=float, default=DEFAULTS['weight_decay'],
        metavar='<float>', help='weight decay [%(default)g]')
    parser.add_argument('--warmup_ratio', type=float, default=DEFAULTS['warmup_ratio'],
        metavar='<float>', help='warmup ratio [%(default)g]')
    parser.add_argument('--grad_accum', type=int, default=DEFAULTS['grad_accum'],
        metavar='<int>', help='gradient accumulation steps [%(default)i]')
    parser.add_argument('--max_grad_norm', type=float, default=DEFAULTS['max_grad_norm'],
        metavar='<float>', help='max gradient norm [%(default)g]')
    parser.add_argument('--max_gen_length', type=int, default=DEFAULTS['max_gen_length'],
        metavar='<int>', help='max generation length [%(default)i]')
    parser.add_argument('--temperature', type=float, default=DEFAULTS['temperature'],
        metavar='<float>', help='generation temperature [%(default)g]')
    parser.add_argument('--top_k', type=int, default=DEFAULTS['top_k'],
        metavar='<int>', help='top-k sampling [%(default)i]')
    parser.add_argument('--top_p', type=float, default=DEFAULTS['top_p'],
        metavar='<float>', help='top-p sampling [%(default)g]')
    parser.add_argument('--exon_weight', type=float, default=DEFAULTS['exon_weight'],
        metavar='<float>', help='exon F1 reward weight [%(default)g]')
    parser.add_argument('--gene_weight', type=float, default=DEFAULTS['gene_weight'],
        metavar='<float>', help='gene F1 reward weight [%(default)g]')
    parser.add_argument('--seed', type=int, default=42,
        metavar='<int>', help='random seed [%(default)i]')
    parser.add_argument('--save_steps', type=int, default=None,
        metavar='<int>', help='save checkpoint every N steps')
    parser.add_argument('--log_every', type=int, default=1,
        metavar='<int>', help='log every N steps [%(default)i]')
    parser.add_argument('--compile', action='store_true',
        help='enable torch.compile on decoder')
    parser.add_argument('--eval_data', type=str, default=None,
        metavar='<file>', help='eval JSON for periodic F1 evaluation')
    parser.add_argument('--eval_samples', type=int, default=20,
        metavar='<int>', help='eval samples per checkpoint [%(default)i]')

    args = parser.parse_args()

    # Distributed setup
    dist_info = train_util.setup_distributed(backend="nccl")
    is_dist   = dist_info is not None
    is_main   = train_util.is_main_process()

    if is_main:
        print(f"\n{' GeneT5 GRPO Training ':=^60}")

    if is_dist and is_main:
        print(f"Distributed: {dist_info['world_size']} nodes, rank {dist_info['rank']}")

    device = train_util.get_device()
    if is_main:
        print(f"Device: {device}")

    # Seeds
    torch.manual_seed(args.seed)
    random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
        torch.set_float32_matmul_precision('high')

    output_dir = pathlib.Path(args.output_dir)
    if is_main:
        output_dir.mkdir(parents=True, exist_ok=True)
    if is_dist:
        train_util.barrier()

    # Load tokenizer
    if is_main:
        print(f"\nLoading tokenizer...")
    tokenizer = tk.GeneTokenizer(pathlib.Path(args.model_path))
    if is_main:
        print(f"  Vocab size: {len(tokenizer)}")

    # Load policy model
    if is_main:
        print(f"\nLoading policy model...")
    policy = mdl.GeneT5.from_pretrained(
        pathlib.Path(args.model_path), device='cpu', dtype=torch.bfloat16
    )

    # Load reference model (frozen copy)
    if is_main:
        print(f"Loading reference model...")
    ref_model = mdl.GeneT5.from_pretrained(
        pathlib.Path(args.model_path), device='cpu', dtype=torch.bfloat16
    )
    ref_model.to(device)
    ref_model.eval()
    for p in ref_model.parameters():
        p.requires_grad = False

    # Compile
    if args.compile:
        if is_main:
            print("\nCompiling model regions...")
        compile_opts = {"mode": "default", "dynamic": None}
        policy.decoder    = torch.compile(policy.decoder, **compile_opts)
        ref_model.decoder = torch.compile(ref_model.decoder, **compile_opts)
        if is_main:
            print("  Compiled: decoder (policy + reference)")

    # Wrap policy with DDP
    policy = train_util.wrap_model_distributed(
        policy, device, find_unused_params=False, static_graph=True
    )

    if is_dist:
        from torch.distributed.algorithms.ddp_comm_hooks import default_hooks
        policy.register_comm_hook(state=None, hook=default_hooks.bf16_compress_hook)

    raw_policy = train_util.unwrap_model(policy)
    stats      = raw_policy.get_param_stats()
    if is_main:
        print(f"  Policy trainable: {stats['total_trainable']:,}")

    # Load dataset
    if is_main:
        print(f"\nLoading GRPO data...")
    dataset = grpo.GRPODataset(args.data, tokenizer)
    if is_main:
        print(f"  Samples: {len(dataset):,}")

    collate_fn   = partial(grpo.grpo_collate, pad_id=tokenizer.pad_token_id)
    dist_sampler = None
    if is_dist:
        dist_sampler = data_utils.distributed.DistributedSampler(dataset, shuffle=True)

    dataloader = data_utils.DataLoader(
        dataset,
        batch_size = args.batch_size,
        shuffle    = not is_dist,
        sampler    = dist_sampler,
        collate_fn = collate_fn,
        num_workers = 2,
        pin_memory  = True,
        drop_last   = True,
    )

    # Optimizer + scheduler
    total_steps  = len(dataloader) // args.grad_accum
    warmup_steps = int(total_steps * args.warmup_ratio)

    effective_lr = args.lr * train_util.get_world_size() if is_dist else args.lr

    optimizer = torch.optim.AdamW(
        [p for p in policy.parameters() if p.requires_grad],
        lr           = effective_lr,
        betas        = (0.9, 0.95),
        weight_decay = args.weight_decay,
        fused        = True,
    )

    scheduler = tf.get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)

    if is_main:
        print(f"\n  Total steps:    {total_steps}")
        print(f"  Warmup steps:   {warmup_steps}")
        print(f"  Effective LR:   {effective_lr}")
        print(f"  Grad accum:     {args.grad_accum}")
        print(f"  Group size:     {args.group_size}")
        print(f"  KL beta:        {args.beta}")
        print(f"  Gen max length: {args.max_gen_length}")
        print(f"  Temperature:    {args.temperature}")

    # Save config
    if is_main:
        config = {
            **vars(args),
            'vocab_size':   len(tokenizer),
            'num_samples':  len(dataset),
            'total_steps':  total_steps,
            'world_size':   train_util.get_world_size(),
            'effective_lr': effective_lr,
        }
        with open(output_dir / 'grpo_config.json', 'w') as f:
            json.dump(config, f, indent=2)

    # Eval hook
    evaluator   = None
    eval_logger = None
    if args.eval_data and is_main:
        evaluator   = util.CheckpointEvaluator(
            args.eval_data, tokenizer, num_samples=args.eval_samples
        )
        eval_logger = util.EvalLogger(output_dir)
        print(f"  Eval data: {args.eval_data} ({len(evaluator.samples)} samples)")

    # Logger
    grpo_logger = None
    if is_main:
        grpo_logger = GRPOLogger(output_dir)
        print(f"  GRPO log: {grpo_logger.log_path}")

    # Training
    if is_main:
        print(f"\n{'=' * 60}")
        print('GRPO Training...')
        print(f"{'=' * 60}")

    dtype        = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    global_step  = 0
    accum_step   = 0
    bos_id       = tokenizer.bos_token_id

    if dist_sampler:
        dist_sampler.set_epoch(0)

    optimizer.zero_grad(set_to_none=True)

    for batch_idx, batch in enumerate(dataloader):
        step_start = time.time()

        input_ids    = batch["input_ids"].to(device)
        ref_features = batch["ref_features"]
        sequences    = batch["sequences"]

        # Build prefix_ids: input DNA tokens + bos separator
        bos_col    = torch.full((input_ids.size(0), 1), bos_id, dtype=torch.long, device=device)
        prefix_ids = torch.cat([input_ids, bos_col], dim=1)

        # ========== Phase 1: Generate (no_grad) ==========
        gen_start = time.time()
        policy.eval()

        with torch.no_grad():
            with torch.amp.autocast('cuda', dtype=dtype):
                generated, expanded_prefix = generate_group(
                    raw_policy, prefix_ids,
                    args.group_size, tokenizer, args
                )

        gen_sec    = time.time() - gen_start
        prefix_len = prefix_ids.size(1)

        # ========== Phase 2: Score (CPU) ==========
        rewards_tensor = score_generations(
            generated, prefix_len, ref_features, sequences,
            args.group_size, tokenizer, args
        )

        # ========== Phase 3: Advantages ==========
        advantages = grpo.compute_advantages(rewards_tensor, args.group_size)
        advantages = advantages.to(device)

        # ========== Phase 4: Policy Update (with grad) ==========
        update_start = time.time()
        policy.train()

        # Prepare teacher-forcing inputs from generated sequences
        tf_input_ids, labels = grpo.prepare_grpo_inputs(
            expanded_prefix, generated, tokenizer.pad_token_id
        )
        tf_input_ids = tf_input_ids.to(device)
        labels       = labels.to(device)

        # Gradient sync only on accumulation boundary
        is_sync_step = (accum_step + 1) % args.grad_accum == 0
        sync_context = nullcontext() if (not is_dist or is_sync_step) else policy.no_sync()

        with sync_context:
            with torch.amp.autocast('cuda', dtype=dtype):
                # Policy log probs
                policy_log_probs, mask, moe_loss = grpo.compute_log_probs(
                    policy, tf_input_ids, labels, raw_policy.vocab_size,
                )

                # Reference log probs
                with torch.no_grad():
                    ref_log_probs, _, _ = grpo.compute_log_probs(
                        ref_model, tf_input_ids, labels, ref_model.vocab_size,
                    )

                # GRPO loss
                loss, policy_loss_val, kl_loss_val, kl_mean = grpo.grpo_loss(
                    policy_log_probs, ref_log_probs,
                    advantages, mask, beta=args.beta,
                )

                # Add MoE auxiliary loss
                moe_loss_val = 0.0
                if moe_loss is not None:
                    loss         = loss + 0.01 * moe_loss
                    moe_loss_val = moe_loss.item()

                loss = loss / args.grad_accum

            loss.backward()

        accum_step += 1

        if accum_step % args.grad_accum == 0:
            torch.nn.utils.clip_grad_norm_(
                [p for p in policy.parameters() if p.requires_grad],
                args.max_grad_norm
            )
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad(set_to_none=True)
            global_step += 1

            update_sec = time.time() - update_start

            # Logging
            if is_main and global_step % args.log_every == 0:
                r_mean = rewards_tensor.mean().item()
                r_std  = rewards_tensor.std().item()
                lr_now = scheduler.get_last_lr()[0]

                print(
                    f"  [{global_step}/{total_steps}] "
                    f"reward: {r_mean:.3f}+/-{r_std:.3f} | "
                    f"policy: {policy_loss_val:.4f} | "
                    f"kl: {kl_mean:.4f} | "
                    f"moe: {moe_loss_val:.4f} | "
                    f"lr: {lr_now:.2e} | "
                    f"gen: {gen_sec:.1f}s | upd: {update_sec:.1f}s"
                )

                if grpo_logger:
                    grpo_logger.log(
                        step        = global_step,
                        total_steps = total_steps,
                        reward_mean = r_mean,
                        reward_std  = r_std,
                        policy_loss = policy_loss_val,
                        kl_loss     = kl_loss_val,
                        kl_mean     = kl_mean,
                        moe_loss    = moe_loss_val,
                        total_loss  = loss.item() * args.grad_accum,
                        lr          = lr_now,
                        gen_sec     = gen_sec,
                        update_sec  = update_sec,
                    )

            # Step checkpoint
            if args.save_steps and global_step % args.save_steps == 0 and is_main:
                save_path = output_dir / f'grpo_step_{global_step}.pt'
                ckpt = {
                    'global_step':      global_step,
                    'model_state_dict': raw_policy.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                }
                torch.save(ckpt, save_path)
                print(f"  Saved: {save_path.name}")

                # Eval
                if evaluator:
                    print(f"  Running eval...")
                    eval_metrics = evaluator.evaluate(raw_policy, device, dtype)
                    eval_logger.log(0, global_step, eval_metrics)
                    print(
                        f"  Eval: exon_F1={eval_metrics['exon_f1']:.4f}  "
                        f"gene_F1={eval_metrics['gene_f1']:.4f}"
                    )

        # Cleanup
        del (input_ids, prefix_ids, generated, expanded_prefix, rewards_tensor,
             advantages, tf_input_ids, labels, policy_log_probs, ref_log_probs,
             mask, loss)

        if is_dist:
            train_util.barrier()

    # Final save
    if is_main:
        print(f"\n{'=' * 60}")
        print('Saving final model...')

        raw_policy.save(output_dir / 'pytorch_model.bin')
        tokenizer.save_pretrained(output_dir)

        model_path = pathlib.Path(args.model_path)
        with open(model_path / 'config.json') as f:
            model_config = json.load(f)
        with open(output_dir / 'config.json', 'w') as f:
            json.dump(model_config, f, indent=2)

        print(f"\n{'=' * 60}")
        print(f"GRPO Complete!")
        print(f"  Output: {output_dir}")
        print(f"  Steps:  {global_step}")
        print(f"{'=' * 60}")

    # Final eval
    if evaluator and is_main:
        print(f"\nFinal evaluation...")
        eval_metrics = evaluator.evaluate(raw_policy, device, dtype)
        eval_logger.log(0, global_step, eval_metrics)
        print(
            f"  Final: exon_F1={eval_metrics['exon_f1']:.4f}  "
            f"gene_F1={eval_metrics['gene_f1']:.4f}"
        )

    if grpo_logger:
        grpo_logger.close()
    if eval_logger:
        eval_logger.close()

    if is_dist:
        train_util.cleanup_distributed()


if __name__ == '__main__':
    main()
