#!/usr/bin/env python3

import sys
import argparse
import pathlib
import json
import concurrent.futures

sys.path.insert(0, str(pathlib.Path(__file__).parent.parent / 'test'))
from lib import calculate_f1, nucleotide_metrics, exon_metrics, gene_metrics


parser = argparse.ArgumentParser(
    description='Evaluate gene predictions using F1 score metrics')
parser.add_argument('reference', type=str, metavar='<reference.gff>',
    help='Reference GFF3 annotation file')
parser.add_argument('predicted', type=str, nargs='+', metavar='<predicted.gff>',
    help='One or more predicted GFF3 files to evaluate')
parser.add_argument('--level', type=str, default='all',
    choices=['nucleotide', 'exon', 'gene', 'all'],
    help='Evaluation level [%(default)s]')
parser.add_argument('--output', '-o', type=str, default=None,
    metavar='<file>', help='Output JSON report file')
parser.add_argument('--parallel', type=int, default=1,
    metavar='<int>', help='Parallel evaluations [%(default)i]')

args = parser.parse_args()

ref_path   = pathlib.Path(args.reference)
pred_paths = [pathlib.Path(p) for p in args.predicted]

if not ref_path.exists():
    print(f"ERROR: Reference file not found: {ref_path}")
    sys.exit(1)

for pred in pred_paths:
    if not pred.exists():
        print(f"ERROR: Predicted file not found: {pred}")
        sys.exit(1)

levels = ['nucleotide', 'exon', 'gene'] if args.level == 'all' else [args.level]

print(f"\n{' F1 Score Evaluation ':=^60}")
print(f"  Reference:  {ref_path}")
print(f"  Predicted:  {len(pred_paths)} file(s)")
print(f"  Levels:     {', '.join(levels)}")


def evaluate_file(pred_path):
    """
    Evaluate a single predicted GFF against reference.
    """

    results = {'file': str(pred_path.name)}

    for level in levels:
        try:
            metrics        = calculate_f1(str(ref_path), str(pred_path), level=level)
            results[level] = metrics
        except Exception as e:
            results[level] = {'error': str(e)}

    return results


print(f"\n{' Evaluating ':=^60}")

all_results = []

if args.parallel > 1 and len(pred_paths) > 1:
    with concurrent.futures.ProcessPoolExecutor(max_workers=args.parallel) as executor:
        futures = {executor.submit(evaluate_file, p): p for p in pred_paths}

        for future in concurrent.futures.as_completed(futures):
            pred = futures[future]
            try:
                result = future.result()
                all_results.append(result)
                print(f"  {pred.name}")
            except Exception as e:
                print(f"  {pred.name}: ERROR - {e}")
                all_results.append({'file': str(pred.name), 'error': str(e)})
else:
    for pred in pred_paths:
        result = evaluate_file(pred)
        all_results.append(result)
        print(f"  {pred.name}")

print(f"\n{' Results ':=^60}")

for result in all_results:
    fname = result['file']
    print(f"\n  {fname}")
    print(f"  {'-' * (len(fname) + 2)}")

    if 'error' in result and isinstance(result['error'], str):
        print(f"    ERROR: {result['error']}")
        continue

    for level in levels:
        if level not in result:
            continue

        m = result[level]
        if 'error' in m:
            print(f"    {level:11s}: ERROR - {m['error']}")
        else:
            f1   = m['f1']
            sens = m['sensitivity']
            prec = m['precision']
            tp   = m['tp']
            fp   = m['fp']
            fn   = m['fn']
            print(f"    {level:11s}: F1={f1:.4f}  Sens={sens:.4f}  Prec={prec:.4f}  (TP={tp} FP={fp} FN={fn})")

if len(all_results) > 1:
    print(f"\n{' Summary ':=^60}")

    for level in levels:
        f1_scores = []
        for r in all_results:
            if level in r and 'f1' in r[level]:
                f1_scores.append(r[level]['f1'])

        if f1_scores:
            avg_f1 = sum(f1_scores) / len(f1_scores)
            min_f1 = min(f1_scores)
            max_f1 = max(f1_scores)
            print(f"  {level:11s}: avg={avg_f1:.4f}  min={min_f1:.4f}  max={max_f1:.4f}  n={len(f1_scores)}")

if args.output:
    output_path = pathlib.Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    report = {
        'reference': str(ref_path),
        'levels':    levels,
        'results':   all_results,
    }

    with open(output_path, 'w') as f:
        json.dump(report, f, indent=2)

    print(f"\n  Report written: {output_path}")

print(f"\n{'='*60}")
print('Done!')
print(f"{'='*60}")
