#!/usr/bin/env python3

import argparse
import json
import pathlib
import random

import torch
import numpy as np

import lib.entropy_model         as entropy
import lib.dataset._fasta_stream as fasta_stream


def build_parser():

    parser = argparse.ArgumentParser(description='Verify BLT dynamic patch compression vs fixed-size')
    parser.add_argument('raw_dir', type=str, metavar='<raw>',
        help='raw FASTA directory with species subdirectories')
    parser.add_argument('--entropy_model', required=True, type=str, metavar='<path>',
        help='trained entropy model checkpoint')
    parser.add_argument('--threshold', type=float, default=1.5, metavar='<float>',
        help='entropy threshold for boundary prediction [%(default)g]')
    parser.add_argument('--min_patch', type=int, default=4, metavar='<int>',
        help='minimum patch size [%(default)i]')
    parser.add_argument('--max_patch', type=int, default=None, metavar='<int>',
        help='maximum patch size (None=no limit) [%(default)s]')
    parser.add_argument('--max_len', type=int, default=512, metavar='<int>',
        help='DNA window length [%(default)i]')
    parser.add_argument('--num_samples', type=int, default=5000, metavar='<int>',
        help='number of samples to analyze [%(default)i]')
    parser.add_argument('--batch_size', type=int, default=32, metavar='<int>',
        help='batch size for entropy computation [%(default)i]')
    parser.add_argument('--seed', type=int, default=42, metavar='<int>',
        help='random seed [%(default)i]')
    return parser


def load_entropy_model(model_path, config_path):
    """Load trained entropy model from checkpoint"""

    with open(config_path) as f:
        cfg = json.load(f)

    model = entropy.DNAEntropyModel(
        vocab_size  = cfg["vocab_size"],
        dim         = cfg["dim"],
        num_layers  = cfg["num_layers"],
        num_heads   = cfg["num_heads"],
        ff_dim      = cfg.get("ff_dim", cfg["dim"] * 4),
        window_size = cfg.get("window_size", 512),
    )
    model.load(model_path)
    return model


def count_bpe_patches(seq_len, patch_size=8):
    """Fixed-size BPE-equivalent: ceil(len / patch_size)"""

    return (seq_len + patch_size - 1) // patch_size


def count_dynamic_patches(entropy_scores, threshold, min_patch, max_patch):
    """Count patches from entropy-guided boundaries"""

    L = len(entropy_scores)
    if L == 0:
        return 0, []

    last_boundary = 0
    patch_sizes   = []

    for i in range(1, L):
        dist = i - last_boundary

        if max_patch is not None and dist >= max_patch:
            patch_sizes.append(dist)
            last_boundary = i
        elif dist >= min_patch and entropy_scores[i] > threshold:
            patch_sizes.append(dist)
            last_boundary = i

    remaining = L - last_boundary
    if remaining > 0:
        patch_sizes.append(remaining)

    return len(patch_sizes), patch_sizes


def main():

    args = build_parser().parse_args()
    random.seed(args.seed)

    print(f"\n{' BLT Compression Verification ':=^60}")

    # load entropy model
    model_dir   = pathlib.Path(args.entropy_model).parent
    model_path  = pathlib.Path(args.entropy_model)
    config_path = model_dir / "entropy_config.json"

    if not model_path.exists():
        print(f"ERROR: Model not found at {model_path}")
        return

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model  = load_entropy_model(str(model_path), str(config_path))
    model  = model.to(device=device, dtype=torch.bfloat16)
    model.eval()

    print(f"  Entropy model: {model.get_param_count():,} params")
    print(f"  Threshold: {args.threshold}")
    print(f"  Patch range: [{args.min_patch}, {args.max_patch or 'unlimited'}]")

    # prepare FASTA data
    raw_dir  = pathlib.Path(args.raw_dir)
    manifest = fasta_stream.prepare_fasta_index(raw_dir)

    if not manifest:
        print("ERROR: No FASTA files found")
        return

    dataset = fasta_stream.FASTAEntropyDataset(
        manifest = manifest,
        max_len  = args.max_len,
        seed     = args.seed,
    )

    # collect samples
    print(f"  Sampling {args.num_samples} windows...")
    samples = []
    for ids in dataset:
        samples.append(ids)
        if len(samples) >= args.num_samples:
            break

    # analyze
    all_entropies   = []
    all_patch_sizes = []
    bpe_patches     = []
    dyn_patches     = []

    for i in range(0, len(samples), args.batch_size):
        batch = samples[i:i + args.batch_size]

        ids_t = torch.tensor(batch, dtype=torch.long, device=device)

        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.bfloat16):
            ent = model.compute_entropy(ids_t)

        for j in range(len(batch)):
            seq_len = len(batch[j])
            seq_ent = ent[j, :seq_len].cpu().numpy()
            all_entropies.extend(seq_ent.tolist())

            bpe_patches.append(count_bpe_patches(seq_len))

            n_patches, psizes = count_dynamic_patches(
                seq_ent, args.threshold, args.min_patch, args.max_patch,
            )
            dyn_patches.append(n_patches)
            all_patch_sizes.extend(psizes)

        if (i // args.batch_size + 1) % 50 == 0:
            print(f"    Processed {min(i + args.batch_size, len(samples))}/{len(samples)}")

    # results
    ent_arr = np.array(all_entropies)
    ps      = np.array(all_patch_sizes)

    avg_bpe = np.mean(bpe_patches)
    avg_dyn = np.mean(dyn_patches)

    improvement = (1.0 - avg_dyn / avg_bpe) * 100 if avg_bpe > 0 else 0

    print(f"\n{'=' * 60}")
    print(f"  Samples analyzed: {len(samples):,}")
    print(f"  Window length:    {args.max_len}")

    print(f"\n  Entropy distribution:")
    print(f"    mean={ent_arr.mean():.4f}, std={ent_arr.std():.4f}")
    print(f"    min={ent_arr.min():.4f}, p25={np.percentile(ent_arr, 25):.4f}, "
          f"median={np.median(ent_arr):.4f}, p75={np.percentile(ent_arr, 75):.4f}, "
          f"max={ent_arr.max():.4f}")

    print(f"\n  Fixed patches (size=8):")
    print(f"    patches/seq: {avg_bpe:,.1f} avg")

    print(f"\n  Dynamic patches (threshold={args.threshold}):")
    print(f"    patches/seq: {avg_dyn:,.1f} avg")

    print(f"\n  Compression improvement: {improvement:+.1f}% fewer global tokens")

    if len(ps) > 0:
        print(f"\n  Patch size distribution:")
        print(f"    min={ps.min()}, p25={int(np.percentile(ps, 25))}, "
              f"median={int(np.median(ps))}, p75={int(np.percentile(ps, 75))}, "
              f"max={ps.max()}")
        print(f"    avg bp/patch: {ps.mean():.1f}")

    print(f"{'=' * 60}")

    if improvement <= 0:
        print("\n  WARNING: Dynamic patches NOT better than fixed patches!")
        print("  Consider: lower threshold, more entropy model training")
    else:
        print(f"\n  PASS: {improvement:.1f}% fewer global tokens with dynamic patches")


if __name__ == '__main__':
    main()
