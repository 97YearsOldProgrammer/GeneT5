#!/usr/bin/env python3

import sys
import os
import argparse
import tempfile
import shutil
import multiprocessing
import pathlib
import subprocess
import concurrent.futures
import json
import random

import lib.util as util


parser = argparse.ArgumentParser(
    description="Bake species data: training bins, validation bins, and eval samples")
parser.add_argument("raw_dir", type=str, metavar="<raw_dir>",
    help="Directory containing species subdirectories (fna.gz + gff.gz)")
parser.add_argument("--output_dir", type=str, default="../baked",
    help="Output directory for baked files [%(default)s]")
parser.add_argument("--log_dir",    type=str, default="../logs/baker",
    help="Directory for log files [%(default)s]")
parser.add_argument("--window_size", type=int, default=20000,
    help="Window size in base pairs [%(default)s]")
parser.add_argument("--species_parallel", type=int, default=3,
    help="Number of species to process in parallel [%(default)s]")
parser.add_argument("--n_workers",  type=int, default=None,
    help="Workers per species for chunking [auto]")
parser.add_argument("--tokenizer",  type=str, required=True,
    help="Tokenizer path (required)")
parser.add_argument("--val_species", type=str, default=None,
    help="Comma-separated held-out species for validation (e.g. B.taurus,S.lycopersicum)")
parser.add_argument("--val_windows", type=int, default=3000,
    help="Max total validation windows from held-out species [%(default)s]")
parser.add_argument("--compress", type=str, default=None,
    choices=['zlib', 'zstd'], help="Compress binary files with zlib or zstd")
parser.add_argument("--nnodes",    type=int, default=1,
    help="Total number of baking nodes [%(default)s]")
parser.add_argument("--node_rank", type=int, default=0,
    help="This node's rank (0-indexed) [%(default)s]")

# Baking mode flags
parser.add_argument("--train", action="store_true",
    help="Bake training.bin + validation.bin (default: on unless --eval only)")
parser.add_argument("--eval", action="store_true",
    help="Generate eval.json from held-out species (default: on unless --train only)")
parser.add_argument("--eval_samples", type=int, default=50,
    help="Eval samples per held-out species [%(default)s]")
parser.add_argument("--seed", type=int, default=42,
    help="Random seed for eval sample selection [%(default)s]")
parser.add_argument("--finalize", action="store_true",
    help="Skip per-species baking, only generate eval (run after distributed bake)")
parser.add_argument("--force", action="store_true",
    help="Re-bake all species even if output already exists")

args = parser.parse_args()

# Finalize mode: merge + eval only, force single-node view
if args.finalize:
    args.train     = True
    args.eval      = True
    args.nnodes    = 1
    args.node_rank = 0
elif not args.train and not args.eval:
    args.train = True
    args.eval  = True

# Parse held-out species
val_species_set = set()
if args.val_species:
    val_species_set = {s.strip() for s in args.val_species.split(",")}

# Inherit held-out split from existing config if not specified
if not args.val_species:
    existing_cfg = pathlib.Path(args.output_dir) / "bake_config.json"
    if existing_cfg.exists():
        with open(existing_cfg) as f:
            prev = json.load(f)
        prev_heldout = prev.get("heldout_species", [])
        if prev_heldout:
            val_species_set = set(prev_heldout)
            print(f"  Inherited held-out from existing config: {', '.join(sorted(val_species_set))}")

n_workers_per_species = args.n_workers or max(1, multiprocessing.cpu_count() - 1)
raw_dir               = pathlib.Path(args.raw_dir)

# Auto-discover species from raw dir
all_discovered = util.discover_species(raw_dir)

if not all_discovered:
    print("No valid species found in raw directory!")
    sys.exit(1)

random.seed(args.seed)


# Measure genome size for load balancing
def _genome_size(name, path):
    fna = pathlib.Path(path) / "fna.gz"
    return fna.stat().st_size if fna.exists() else 0


# Split held-out vs training species
train_species   = [(name, path) for name, path in all_discovered if name not in val_species_set]
heldout_species = [(name, path) for name, path in all_discovered if name in val_species_set]

# Greedy bin-packing for training species only
train_species.sort(key=lambda sp: _genome_size(*sp), reverse=True)
bins      = [[] for _ in range(args.nnodes)]
bin_sizes = [0]  * args.nnodes

for sp in train_species:
    lightest          = bin_sizes.index(min(bin_sizes))
    bins[lightest].append(sp)
    bin_sizes[lightest] += _genome_size(*sp)

species_to_process = bins[args.node_rank]

# Node 0 also handles held-out species
if args.node_rank == 0:
    heldout_to_process = heldout_species
else:
    heldout_to_process = []

output_dir = pathlib.Path(args.output_dir)
log_dir    = pathlib.Path(args.log_dir)

output_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)

# Detect already-baked species (tar shards for training, binary for held-out)
already_baked = set()
if not args.force and not args.finalize:
    for d in output_dir.iterdir():
        if not d.is_dir():
            continue
        has_tar    = any(d.glob("*.tar"))
        has_binary = (d / "training.bin").exists()
        if has_tar or has_binary:
            already_baked.add(d.name)

    if already_baked:
        species_to_process = [(n, p) for n, p in species_to_process if n not in already_baked]
        heldout_to_process = [(n, p) for n, p in heldout_to_process if n not in already_baked]

if not species_to_process and not heldout_to_process and not already_baked:
    print("No species found!")
    sys.exit(0)


####################
#####  Header  #####
####################


modes = []
if args.finalize:
    modes.append("finalize")
elif args.train:
    modes.append("train")
if args.eval and not args.finalize:
    modes.append("eval")

print(f"\n{'='*60}")
print(f"{'GeneT5 Data Baker':^60}")
print(f"{'='*60}")
print(f"  Raw directory:    {raw_dir}")
print(f"  Output directory: {output_dir}")
print(f"  Tokenizer:        {args.tokenizer} (fast)")
print(f"  Mode:             {' + '.join(modes)}")
print(f"")
print(f"  Window size:      {args.window_size:,} bp")
print(f"  Canonical only:   True")
if val_species_set:
    print(f"  Held-out species: {', '.join(sorted(val_species_set))}")
if args.eval:
    print(f"  Eval samples:     {args.eval_samples} per held-out species")
print(f"")
if args.nnodes > 1:
    total_size = sum(_genome_size(*s) for s in species_to_process)
    print(f"  Node:             {args.node_rank} of {args.nnodes}")
    print(f"  This node:        {len(species_to_process)} species ({total_size / 1024**3:.1f} GB)")
    print(f"  Total species:    {len(train_species)}")
else:
    print(f"  Species:          {len(all_discovered)}")
print(f"  Species parallel: {args.species_parallel}")
print(f"  Workers/species:  {n_workers_per_species}")

print(f"\n{' Species Summary ':=^60}")
print(f"  Discovered:     {len(all_discovered)} species")
if already_baked:
    print(f"  Already baked:  {len(already_baked)} (use --force to re-bake)")
print(f"  New to bake:    {len(species_to_process)} training + {len(heldout_to_process)} held-out")
if species_to_process:
    for name, _ in sorted(species_to_process):
        print(f"    + {name}")
elif not args.finalize:
    print(f"    (nothing new)")


##############################
#####  Process Species   #####
##############################


def _make_jobs(species_list, output_format="binary"):
    """Build BakeJob list from (name, path) tuples"""

    return [
        util.BakeJob(
            species       = name,
            raw_dir       = str(raw_dir),
            output_dir    = str(output_dir),
            log_dir       = str(log_dir),
            window_size   = args.window_size,
            tokenizer     = args.tokenizer,
            n_workers     = n_workers_per_species,
            compress      = args.compress,
            output_format = output_format,
        )
        for name, path in species_list
    ]


if not args.finalize and args.train:
    work_items = _make_jobs(species_to_process, output_format="tar")

    print(f"\n{' Processing Training Species ':=^60}")

    results      = []
    success      = 0
    failed       = 0
    train_totals = {}

    with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
        futures = {executor.submit(util.process_species, job): job.species for job in work_items}

        for future in concurrent.futures.as_completed(futures):
            species_name = futures[future]
            try:
                result = future.result()
                results.append(result)

                if result["success"]:
                    success += 1
                    n = result.get("total_samples", 0)
                    if n:
                        train_totals[species_name] = n
                    print(f"  + {species_name} ({n:,} samples)")
                else:
                    failed += 1
                    error = result.get("error", "Unknown error")
                    print(f"  x {species_name}: {error}")

            except Exception as e:
                failed += 1
                results.append({"species": species_name, "success": False, "error": str(e)})
                print(f"  x {species_name}: {e}")

    print(f"\n{' Processing Results ':=^60}")
    print(f"  Success: {success}")
    print(f"  Failed:  {failed}")


#####################################
#####  Held-Out Species (Val)   #####
#####################################


if not args.finalize and heldout_to_process and args.train:
    print(f"\n{' Held-Out Validation Species ':=^60}")

    heldout_items = _make_jobs(heldout_to_process, output_format="binary")

    with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
        futures = {executor.submit(util.process_species, job): job.species for job in heldout_items}

        for future in concurrent.futures.as_completed(futures):
            species_name = futures[future]
            try:
                result = future.result()
                if result["success"]:
                    print(f"  + {species_name} (held-out)")
                else:
                    error = result.get("error", "Unknown error")
                    print(f"  x {species_name}: {error}")

            except Exception as e:
                print(f"  x {species_name}: {e}")


##############################
#####  Merge Outputs     #####
##############################


if args.train and (args.finalize or args.nnodes == 1):
    import lib.dataset as ds

    all_train_names   = {name for name, _ in train_species}
    all_heldout_names = {name for name, _ in heldout_species}

    # Discover tar shards for training species
    all_tar_files  = sorted(output_dir.glob("*/*.tar"))
    training_tars  = [f for f in all_tar_files if f.parent.name in all_train_names]

    # Count total training samples from stats.json (written by parse_data.py)
    total_train_samples = 0
    for sp_name in all_train_names:
        stats_path = output_dir / sp_name / "stats.json"
        if stats_path.exists():
            with open(stats_path) as f:
                sp_stats = json.load(f)
            total_train_samples += sp_stats.get("run_stats", {}).get("total_samples", 0)

    print(f"\n{' Per-Species Training Shards (tar) ':=^60}")
    total_bytes = 0
    species_tar_counts = {}
    for f in training_tars:
        fsize        = f.stat().st_size
        total_bytes += fsize
        sp           = f.parent.name
        species_tar_counts[sp] = species_tar_counts.get(sp, 0) + 1

    for sp in sorted(species_tar_counts):
        n_tars = species_tar_counts[sp]
        sp_size = sum(f.stat().st_size for f in training_tars if f.parent.name == sp)
        print(f"    {sp:30s} {n_tars:>3} tar(s)  ({sp_size / 1024 / 1024:.1f} MB)")

    if total_bytes < 1024 ** 3:
        total_str = f"{total_bytes / 1024 / 1024:.1f} MB"
    else:
        total_str = f"{total_bytes / 1024 ** 3:.2f} GB"
    print(f"  Total: {len(training_tars)} tars, {total_train_samples:,} samples, {total_str}")

    # Validation merge from held-out species (still binary)
    validation_bins = sorted(
        f for f in output_dir.glob("*/training.bin")
        if f.parent.name in all_heldout_names
    )
    if validation_bins:
        print(f"\n  Validation merge (capped at {args.val_windows:,}):")
        ds.merge_binary_files(
            validation_bins,
            output_dir / "validation.bin",
            compress      = args.compress,
            show_progress = True,
            max_chunks    = args.val_windows,
        )


#####################################
#####  Generate Eval Samples    #####
#####################################


all_eval_samples = []

if heldout_species and args.eval and (args.finalize or args.nnodes == 1):
    import lib.dataset as ds

    print(f"\n{' Generating Eval Samples ':=^60}")

    for sp, _ in heldout_species:
        species_dir = raw_dir / sp

        # Try loading gene_index sidecar first (avoids re-parsing GFF)
        sidecar_path = output_dir / sp / "gene_index.json"
        gene_index   = ds.load_gene_index(sidecar_path)

        if gene_index is not None:
            print(f"  {sp}: loaded gene_index from sidecar")
        else:
            print(f"  {sp}: WARNING - no sidecar, re-parsing GFF")
            fna_path, gff_path = ds.find_genome_files(species_dir)
            features   = ds.parse_gff(gff_path)
            gene_index = ds.build_gene_index(features)
            gene_index = ds.filter_canonical_transcripts(gene_index)

        # FASTA still needs to be read for eval (need actual sequences)
        fna_path, _ = ds.find_genome_files(species_dir)
        sequences   = ds.parse_fasta(fna_path)
        coding      = ds.extract_coding_genes(gene_index)

        species_samples = []
        for gene_id, gene_data in coding.items():
            sample = ds.build_eval_sample(gene_id, gene_data, sequences, args.window_size)
            if sample:
                sample["species"] = sp
                species_samples.append(sample)

        selected = ds.select_diverse_samples(species_samples, args.eval_samples)
        all_eval_samples.extend(selected)
        print(f"  {sp}: {len(selected)} eval samples from {len(species_samples)} candidates")

    # Write eval.json
    if all_eval_samples:
        eval_path = output_dir / "eval.json"
        with open(eval_path, 'w') as f:
            json.dump(all_eval_samples, f)

        total_exons = sum(s["num_exons"] for s in all_eval_samples)
        size_mb     = eval_path.stat().st_size / 1024 / 1024
        print(f"\n  eval.json: {len(all_eval_samples)} samples, {total_exons} exons ({size_mb:.1f} MB)")


##############################
#####  Bake Config       #####
##############################


# Ensure total_train_samples is set even if merge section was skipped
if 'total_train_samples' not in dir():
    total_train_samples = 0

bake_config = {
    **{k: str(v) if isinstance(v, pathlib.Path) else v for k, v in vars(args).items()},
    "train_species":        [name for name, _ in train_species],
    "heldout_species":      [name for name, _ in heldout_species],
    "eval_samples_generated": len(all_eval_samples),
    "output_format":        "tar",
    "total_train_samples":  total_train_samples,
}

if args.nnodes > 1 and not args.finalize:
    config_path = output_dir / f"bake_config_node{args.node_rank}.json"
else:
    config_path = output_dir / "bake_config.json"
with open(config_path, 'w') as f:
    json.dump(bake_config, f, indent=2)


##############################
#####  Final Summary     #####
##############################


print(f"\n{' Final Output ':=^60}")

if args.train:
    import lib.dataset as ds

    merged_val = output_dir / "validation.bin"
    tar_count  = len(list(output_dir.glob("*/*.tar")))

    if tar_count > 0:
        print(f"  training tars:   {tar_count} shards, {total_train_samples:,} samples")

    if merged_val.exists():
        info = ds.get_binary_info(merged_val)
        size = ds.format_size(merged_val.stat().st_size)
        print(f"  validation.bin:  {info['num_chunks']:,} chunks  ({size})")

if all_eval_samples:
    print(f"  eval.json:       {len(all_eval_samples)} samples")

print(f"  bake_config.json")

print(f"\n{'='*60}")
print("Done!")
print(f"{'='*60}")
