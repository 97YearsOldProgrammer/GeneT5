#!/usr/bin/env python3

import sys
import argparse
import tempfile
import shutil
import multiprocessing
import pathlib
import subprocess
import concurrent.futures
import json

import lib.util as util


parser = argparse.ArgumentParser(
    description="Bake all species data: parse, tokenize, compact")
parser.add_argument("--raw_dir",    type=str, default="../raw",
    help="Directory containing species subdirectories [%(default)s]")
parser.add_argument("--output_dir", type=str, default="../baked",
    help="Output directory for final .packed files [%(default)s]")
parser.add_argument("--log_dir",    type=str, default="../logs/baker",
    help="Directory for log files [%(default)s]")
parser.add_argument("--taxa",       type=str, nargs='+', default=None,
    help="Process only specific taxa (default: all)")
parser.add_argument("--species",    type=str, nargs='+', default=None,
    help="Process only specific species (default: all)")
parser.add_argument("--species_parallel", type=int, default=2,
    help="Number of species to process in parallel [%(default)s]")
parser.add_argument("--n_workers",  type=int, default=None,
    help="Workers per species for chunking [auto]")
parser.add_argument("--tokenizer",  type=str, required=True,
    help="Tokenizer path (required)")
parser.add_argument("--target",     type=int, default=20000,
    help="Target tokens per packed sequence [%(default)s]")
parser.add_argument("--hard_limit", type=int, default=22000,
    help="Hard limit for packing (default: target * 1.1)")
parser.add_argument("--file_parallel", type=int, default=5,
    help="Files to process in parallel during compacting [%(default)s]")
parser.add_argument("--compact_workers", type=int, default=8,
    help="Parallel workers for compacting phase [%(default)s]")
parser.add_argument("--num_complex", type=int, default=7,
    help="Number of complex genes for validation per species [%(default)s]")
parser.add_argument("--num_normal",  type=int, default=7,
    help="Number of normal genes for validation per species [%(default)s]")
parser.add_argument("--num_easy",    type=int, default=7,
    help="Number of easy genes for validation per species [%(default)s]")
parser.add_argument("--keep_temp",  action="store_true",
    help="Keep temporary per-species files (for debugging)")

args = parser.parse_args()

n_workers_per_species = args.n_workers or max(1, multiprocessing.cpu_count() - 1)
species_to_process    = util.build_species_list(args.species, args.taxa)

if not species_to_process:
    print("No species to process!")
    sys.exit(0)

raw_dir    = pathlib.Path(args.raw_dir)
output_dir = pathlib.Path(args.output_dir)
log_dir    = pathlib.Path(args.log_dir)

output_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)

temp_dir = pathlib.Path(tempfile.mkdtemp(prefix="bake_"))


####################
#####  Header  #####
####################


print(f"\n{'='*60}")
print(f"{'GeneT5 Data Baker':^60}")
print(f"{'='*60}")
print(f"  Raw directory:    {raw_dir}")
print(f"  Output directory: {output_dir}")
print(f"  Temp directory:   {temp_dir}")
print(f"  Tokenizer:        {args.tokenizer}")
print(f"  Target tokens:    {args.target:,}")
print(f"  Species parallel: {args.species_parallel}")
print(f"  Workers/species:  {n_workers_per_species}")
print(f"  Compact workers:  {args.compact_workers}")
print(f"  Species:          {len(species_to_process)}")

print(f"\n{' Taxa Summary ':=^60}")

taxa_counts = {}
for sp, limit, taxa in species_to_process:
    if taxa not in taxa_counts:
        taxa_counts[taxa] = {"count": 0, "limit": limit}
    taxa_counts[taxa]["count"] += 1

for taxa, info in taxa_counts.items():
    print(f"  {taxa:15s}: {info['count']:2d} species @ {info['limit']:,} bp")


##############################
#####  Process Species   #####
##############################


work_items = [
    (sp, raw_dir, temp_dir, log_dir, limit, None, args.tokenizer,
     n_workers_per_species, args.num_complex, args.num_normal, args.num_easy)
    for sp, limit, taxa in species_to_process
]

print(f"\n{' Processing Species ':=^60}")

results = []
success = 0
failed  = 0

with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
    futures = {executor.submit(util.process_species, item): item[0] for item in work_items}

    for future in concurrent.futures.as_completed(futures):
        species_name = futures[future]
        try:
            result = future.result()
            results.append(result)

            if result["success"]:
                success += 1
                print(f"  ✓ {species_name}")
            else:
                failed += 1
                error = result.get("error", "Unknown error")
                print(f"  ✗ {species_name}: {error}")

        except Exception as e:
            failed += 1
            results.append({"species": species_name, "success": False, "error": str(e)})
            print(f"  ✗ {species_name}: {e}")

print(f"\n{' Processing Results ':=^60}")
print(f"  Success: {success}")
print(f"  Failed:  {failed}")

if success == 0:
    print("\nNo species processed successfully. Exiting.")
    if not args.keep_temp:
        shutil.rmtree(temp_dir)
    sys.exit(1)


##############################
#####  Compact Training  #####
##############################


script_dir     = pathlib.Path(__file__).parent.resolve()
compact_script = script_dir / "compact.py"

print(f"\n{' Compacting Training ':=^60}")

training_files = list(temp_dir.glob("*/training.bin"))

if not training_files:
    print("  No training.bin files found!")
else:
    print(f"  Found {len(training_files)} training files")

    output_path = output_dir / "training.packed"

    compact_cmd = [
        "python3", str(compact_script),
    ] + [str(f) for f in training_files] + [
        "-o", str(output_path),
        "--tokenizer", args.tokenizer,
        "--target", str(args.target),
        "--file_parallel", str(args.file_parallel),
        "--workers", str(args.compact_workers),
    ]

    if args.hard_limit:
        compact_cmd += ["--hard_limit", str(args.hard_limit)]

    result = subprocess.run(compact_cmd, capture_output=True, text=True)

    if result.returncode == 0:
        print(f"  ✓ Training compacting complete")
        output_size = output_path.stat().st_size if output_path.exists() else 0
        print(f"    Output: {output_path}")
        print(f"    Size:   {output_size / 1024 / 1024:.2f} MB")
    else:
        print(f"  ✗ Training compacting failed")
        if result.stderr:
            for line in result.stderr.strip().split('\n')[-10:]:
                print(f"    {line}")


##############################
#####  Compact Validation ####
##############################


print(f"\n{' Compacting Validation ':=^60}")

validation_files = list(temp_dir.glob("*/validation.bin"))

if not validation_files:
    print("  No validation.bin files found!")
else:
    print(f"  Found {len(validation_files)} validation files")

    val_output_path = output_dir / "validation.packed"

    val_compact_cmd = [
        "python3", str(compact_script),
    ] + [str(f) for f in validation_files] + [
        "-o", str(val_output_path),
        "--tokenizer", args.tokenizer,
        "--target", str(args.target),
        "--file_parallel", str(args.file_parallel),
        "--workers", str(args.compact_workers),
    ]

    if args.hard_limit:
        val_compact_cmd += ["--hard_limit", str(args.hard_limit)]

    val_result = subprocess.run(val_compact_cmd, capture_output=True, text=True)

    if val_result.returncode == 0:
        print(f"  ✓ Validation compacting complete")
        val_size = val_output_path.stat().st_size if val_output_path.exists() else 0
        print(f"    Output: {val_output_path}")
        print(f"    Size:   {val_size / 1024 / 1024:.2f} MB")
    else:
        print(f"  ✗ Validation compacting failed")
        if val_result.stderr:
            for line in val_result.stderr.strip().split('\n')[-10:]:
                print(f"    {line}")


##############################
#####  Cleanup           #####
##############################


if args.keep_temp:
    print(f"\n  Temp files kept at: {temp_dir}")
else:
    shutil.rmtree(temp_dir)
    print(f"\n  Temp files cleaned up")


##############################
#####  Aggregate Stats   #####
##############################


print(f"\n{' Aggregating Stats ':=^60}")

agg_stats = {
    'windows': {
        'windows_scanned': 0,
        'windows_empty':   0,
        'windows_n_heavy': 0,
        'windows_kept':    0,
    },
    'samples': {
        'raw_count':     0,
        'aug_count':     0,
        'total_samples': 0,
    },
}

# Note: stats are in temp_dir which may be deleted
# For now just show final output

print(f"\n{' Final Output ':=^60}")

train_path = output_dir / "training.packed"
val_path   = output_dir / "validation.packed"

if train_path.exists():
    print(f"  Training:   {train_path} ({train_path.stat().st_size / 1024 / 1024:.2f} MB)")
if val_path.exists():
    print(f"  Validation: {val_path} ({val_path.stat().st_size / 1024 / 1024:.2f} MB)")

print(f"\n{'='*60}")
print("Done!")
print(f"{'='*60}")
