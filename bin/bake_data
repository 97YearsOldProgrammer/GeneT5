#!/usr/bin/env python3

import sys
import os
import argparse
import tempfile
import shutil
import multiprocessing
import pathlib
import subprocess
import concurrent.futures
import json

import lib.util as util


parser = argparse.ArgumentParser(
    description="Bake all species data: parse and tokenize")
parser.add_argument("--raw_dir",    type=str, default="../raw",
    help="Directory containing species subdirectories [%(default)s]")
parser.add_argument("--output_dir", type=str, default="../baked",
    help="Output directory for baked .bin files [%(default)s]")
parser.add_argument("--log_dir",    type=str, default="../logs/baker",
    help="Directory for log files [%(default)s]")
parser.add_argument("--taxa",       type=str, nargs='+', default=None,
    help="Process only specific taxa (default: all)")
parser.add_argument("--species",    type=str, nargs='+', default=None,
    help="Process only specific species (default: all)")
parser.add_argument("--window_size", type=int, default=200000,
    help="Window size in base pairs [%(default)s]")
parser.add_argument("--species_parallel", type=int, default=3,
    help="Number of species to process in parallel [%(default)s]")
parser.add_argument("--n_workers",  type=int, default=None,
    help="Workers per species for chunking [auto]")
parser.add_argument("--tokenizer",  type=str, required=True,
    help="Tokenizer path (required)")
parser.add_argument("--num_complex", type=int, default=10,
    help="Number of complex genes for validation per species [%(default)s]")
parser.add_argument("--num_normal",  type=int, default=10,
    help="Number of normal genes for validation per species [%(default)s]")
parser.add_argument("--num_easy",    type=int, default=10,
    help="Number of easy genes for validation per species [%(default)s]")
parser.add_argument("--compress", type=str, default=None,
    choices=['zlib', 'zstd'], help="Compress binary files with zlib or zstd")
parser.add_argument("--canonical_only", action="store_true",
    help="Keep only canonical (longest) transcript per gene")

args = parser.parse_args()

n_workers_per_species = args.n_workers or max(1, multiprocessing.cpu_count() - 1)
species_to_process    = util.build_species_list(args.species, args.taxa, args.window_size)

if not species_to_process:
    print("No species to process!")
    sys.exit(0)

raw_dir    = pathlib.Path(args.raw_dir)
output_dir = pathlib.Path(args.output_dir)
log_dir    = pathlib.Path(args.log_dir)

output_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)


####################
#####  Header  #####
####################


print(f"\n{'='*60}")
print(f"{'GeneT5 Data Baker':^60}")
print(f"{'='*60}")
print(f"  Raw directory:    {raw_dir}")
print(f"  Output directory: {output_dir}")
print(f"  Tokenizer:        {args.tokenizer}")
print(f"")
print(f"  Window size:      {args.window_size:,} bp")
print(f"  Canonical only:   {args.canonical_only}")
print(f"")
print(f"  Species:          {len(species_to_process)}")
print(f"  Species parallel: {args.species_parallel}")
print(f"  Workers/species:  {n_workers_per_species}")

print(f"\n{' Taxa Summary ':=^60}")

taxa_counts = {}
for sp, limit, taxa in species_to_process:
    taxa_counts[taxa] = taxa_counts.get(taxa, 0) + 1

for taxa, count in taxa_counts.items():
    print(f"  {taxa:15s}: {count:2d} species")


##############################
#####  Process Species   #####
##############################


work_items = [
    (sp, raw_dir, output_dir, log_dir, limit, None, args.tokenizer,
     n_workers_per_species, args.num_complex, args.num_normal, args.num_easy,
     args.compress, args.canonical_only)
    for sp, limit, taxa in species_to_process
]

print(f"\n{' Processing Species ':=^60}")

results = []
success = 0
failed  = 0

with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
    futures = {executor.submit(util.process_species, item): item[0] for item in work_items}

    for future in concurrent.futures.as_completed(futures):
        species_name = futures[future]
        try:
            result = future.result()
            results.append(result)

            if result["success"]:
                success += 1
                print(f"  + {species_name}")
            else:
                failed += 1
                error = result.get("error", "Unknown error")
                print(f"  x {species_name}: {error}")

        except Exception as e:
            failed += 1
            results.append({"species": species_name, "success": False, "error": str(e)})
            print(f"  x {species_name}: {e}")

print(f"\n{' Processing Results ':=^60}")
print(f"  Success: {success}")
print(f"  Failed:  {failed}")


##############################
#####  Merge Outputs     #####
##############################


print(f"\n{' Final Output ':=^60}")

# List all per-species training/validation bins
training_bins   = sorted(output_dir.glob("*/training.bin"))
validation_bins = sorted(output_dir.glob("*/validation.bin"))

print(f"  Training files:   {len(training_bins)}")
print(f"  Validation files: {len(validation_bins)}")

for f in training_bins:
    size_mb = f.stat().st_size / 1024 / 1024
    print(f"    {f.parent.name:30s} {size_mb:.1f} MB")

print(f"\n{'='*60}")
print("Done!")
print(f"{'='*60}")
