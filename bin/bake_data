#!/usr/bin/env python3

import sys
import argparse
import tempfile
import shutil
import multiprocessing
import pathlib
import concurrent.futures

import lib.util as util

parser = argparse.ArgumentParser(
    description="Bake all species data: parse, tokenize, compact")
parser.add_argument("--raw_dir",    type=str, default="../raw",
    help="Directory containing species subdirectories [%(default)s]")
parser.add_argument("--output_dir", type=str, default="../baked",
    help="Output directory for final training.bin and validation.bin [%(default)s]")
parser.add_argument("--log_dir",    type=str, default="../logs/baker",
    help="Directory for log files [%(default)s]")
parser.add_argument("--taxa",       type=str, nargs='+', default=None,
    help="Process only specific taxa (default: all)")
parser.add_argument("--species",    type=str, nargs='+', default=None,
    help="Process only specific species (default: all)")
parser.add_argument("--species_parallel", type=int, default=2,
    help="Number of species to process in parallel [%(default)s]")
parser.add_argument("--n_workers",  type=int, default=None,
    help="Workers per species for chunking [auto]")
parser.add_argument("--tokenizer",  type=str, required=True,
    help="Tokenizer path (required for tokenization)")
parser.add_argument("--compact_target",     type=int, default=15000,
    help="Target tokens for compacting [%(default)s]")
parser.add_argument("--compact_hard_limit", type=int, default=16500,
    help="Hard limit for compacting (default: target * 1.1)")
parser.add_argument("--compact_file_parallel", type=int, default=5,
    help="Files to process in parallel during compacting [%(default)s]")
parser.add_argument("--num_complex", type=int, default=7,
    help="Number of complex genes for validation per species [%(default)s]")
parser.add_argument("--num_normal",  type=int, default=7,
    help="Number of normal genes for validation per species [%(default)s]")
parser.add_argument("--num_easy",    type=int, default=7,
    help="Number of easy genes for validation per species [%(default)s]")
parser.add_argument("--keep_temp",  action="store_true",
    help="Keep temporary per-species files (for debugging)")

args = parser.parse_args()

n_workers_per_species = args.n_workers or max(1, multiprocessing.cpu_count() - 1)
species_to_process    = util.build_species_list(args.species, args.taxa)

if not species_to_process:
    print("No species to process!")
    sys.exit(0)

raw_dir    = pathlib.Path(args.raw_dir)
output_dir = pathlib.Path(args.output_dir)
log_dir    = pathlib.Path(args.log_dir)

output_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)

# Create temp directory for intermediate files
temp_dir = pathlib.Path(tempfile.mkdtemp(prefix="bake_"))

print(f"\n{' GeneT5 Data Baker ':=^60}")
print(f"  Raw directory:    {raw_dir}")
print(f"  Output directory: {output_dir}")
print(f"  Temp directory:   {temp_dir}")
print(f"  Tokenizer:        {args.tokenizer}")
print(f"  Compact target:   {args.compact_target:,} tokens")
print(f"  Species parallel: {args.species_parallel}")
print(f"  Workers/species:  {n_workers_per_species}")
print(f"  Species:          {len(species_to_process)}")

print(f"\n{' Taxa Summary ':=^60}")
taxa_counts = {}
for sp, limit, taxa in species_to_process:
    if taxa not in taxa_counts:
        taxa_counts[taxa] = {"count": 0, "limit": limit}
    taxa_counts[taxa]["count"] += 1

for taxa, info in taxa_counts.items():
    print(f"  {taxa:15s}: {info['count']:2d} species @ {info['limit']:,} bp")

# Process species - output to temp directory
work_items = [
    (sp, raw_dir, temp_dir, log_dir, limit, None, args.tokenizer, n_workers_per_species, args.num_complex, args.num_normal, args.num_easy)
    for sp, limit, taxa in species_to_process
]

print(f"\n{' Processing Species ':=^60}")

results = []
success = 0
failed  = 0

with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
    futures = {executor.submit(util.process_species, item): item[0] for item in work_items}

    for future in concurrent.futures.as_completed(futures):
        species_name = futures[future]
        try:
            result = future.result()
            results.append(result)

            if result["success"]:
                success += 1
                print(f"  ✓ {species_name}")
            else:
                failed += 1
                error = result.get("error", "Unknown error")
                print(f"  ✗ {species_name}: {error}")

        except Exception as e:
            failed += 1
            results.append({"species": species_name, "success": False, "error": str(e)})
            print(f"  ✗ {species_name}: {e}")

print(f"\n{' Processing Results ':=^60}")
print(f"  Success: {success}")
print(f"  Failed:  {failed}")

if success == 0:
    print("\nNo species processed successfully. Exiting.")
    if not args.keep_temp:
        shutil.rmtree(temp_dir)
    sys.exit(1)

# Compact training and validation
script_dir     = pathlib.Path(__file__).parent.resolve()
compact_script = script_dir / "compact.py"

import subprocess

# Compact training files
print(f"\n{' Compacting Training ':=^60}")

training_files = list(temp_dir.glob("*/training.bin"))

if not training_files:
    print("  No training.bin files found!")
else:
    print(f"  Found {len(training_files)} training files")

    output_path = output_dir / "training.bin"
    compact_cmd = [
        "python3", str(compact_script),
    ] + [str(f) for f in training_files] + [
        "-o", str(output_path),
        "--compact_target", str(args.compact_target),
        "--file_parallel", str(args.compact_file_parallel),
    ]
    if args.compact_hard_limit:
        compact_cmd += ["--hard_limit", str(args.compact_hard_limit)]

    result = subprocess.run(compact_cmd, capture_output=True, text=True)

    if result.returncode == 0:
        print(f"  ✓ Training compacting complete")
        if result.stdout:
            lines = result.stdout.strip().split('\n')
            for line in lines[-5:]:
                print(f"    {line}")
        output_size = output_path.stat().st_size if output_path.exists() else 0
        print(f"    Output: {output_path}")
        print(f"    Size:   {output_size / 1024 / 1024:.2f} MB")
    else:
        print(f"  ✗ Training compacting failed")
        if result.stderr:
            print(f"    {result.stderr}")

# Compact validation files
print(f"\n{' Compacting Validation ':=^60}")

validation_files = list(temp_dir.glob("*/validation.bin"))

if not validation_files:
    print("  No validation.bin files found!")
else:
    print(f"  Found {len(validation_files)} validation files")

    val_output_path = output_dir / "validation.bin"
    val_compact_cmd = [
        "python3", str(compact_script),
    ] + [str(f) for f in validation_files] + [
        "-o", str(val_output_path),
        "--compact_target", str(args.compact_target),
        "--file_parallel", str(args.compact_file_parallel),
    ]
    if args.compact_hard_limit:
        val_compact_cmd += ["--hard_limit", str(args.compact_hard_limit)]

    val_result = subprocess.run(val_compact_cmd, capture_output=True, text=True)

    if val_result.returncode == 0:
        print(f"  ✓ Validation compacting complete")
        if val_result.stdout:
            lines = val_result.stdout.strip().split('\n')
            for line in lines[-5:]:
                print(f"    {line}")
        val_size = val_output_path.stat().st_size if val_output_path.exists() else 0
        print(f"    Output: {val_output_path}")
        print(f"    Size:   {val_size / 1024 / 1024:.2f} MB")
    else:
        print(f"  ✗ Validation compacting failed")
        if val_result.stderr:
            print(f"    {val_result.stderr}")

# Cleanup temp directory
if args.keep_temp:
    print(f"\n  Temp files kept at: {temp_dir}")
else:
    shutil.rmtree(temp_dir)
    print(f"\n  Temp files cleaned up")

# Summary
print(f"\n{' Final Output ':=^60}")
train_path = output_dir / "training.bin"
val_path   = output_dir / "validation.bin"

if train_path.exists():
    print(f"  Training:   {train_path} ({train_path.stat().st_size / 1024 / 1024:.2f} MB)")
if val_path.exists():
    print(f"  Validation: {val_path} ({val_path.stat().st_size / 1024 / 1024:.2f} MB)")

print(f"\n{'='*60}")
print("Done!")
print(f"{'='*60}")
