#!/usr/bin/env python3

import sys
import os
import argparse
import tempfile
import shutil
import multiprocessing
import pathlib
import subprocess
import concurrent.futures
import json
import random

import lib.util as util


parser = argparse.ArgumentParser(
    description="Bake species data: training bins, validation bins, and eval samples")
parser.add_argument("raw_dir", type=str, metavar="<raw_dir>",
    help="Directory containing species subdirectories (fna.gz + gff.gz)")
parser.add_argument("--output_dir", type=str, default="../baked",
    help="Output directory for baked files [%(default)s]")
parser.add_argument("--log_dir",    type=str, default="../logs/baker",
    help="Directory for log files [%(default)s]")
parser.add_argument("--window_size", type=int, default=20000,
    help="Window size in base pairs [%(default)s]")
parser.add_argument("--species_parallel", type=int, default=3,
    help="Number of species to process in parallel [%(default)s]")
parser.add_argument("--n_workers",  type=int, default=None,
    help="Workers per species for chunking [auto]")
parser.add_argument("--tokenizer",  type=str, required=True,
    help="Tokenizer path (required)")
parser.add_argument("--val_species", type=str, default=None,
    help="Comma-separated held-out species for validation (e.g. B.taurus,S.lycopersicum)")
parser.add_argument("--val_windows", type=int, default=3000,
    help="Max total validation windows from held-out species [%(default)s]")
parser.add_argument("--compress", type=str, default=None,
    choices=['zlib', 'zstd'], help="Compress binary files with zlib or zstd")
parser.add_argument("--nnodes",    type=int, default=1,
    help="Total number of baking nodes [%(default)s]")
parser.add_argument("--node_rank", type=int, default=0,
    help="This node's rank (0-indexed) [%(default)s]")

# Baking mode flags
parser.add_argument("--train", action="store_true",
    help="Bake training.bin + validation.bin (default: on unless --eval only)")
parser.add_argument("--eval", action="store_true",
    help="Generate eval.json from held-out species (default: on unless --train only)")
parser.add_argument("--eval_samples", type=int, default=50,
    help="Eval samples per held-out species [%(default)s]")
parser.add_argument("--seed", type=int, default=42,
    help="Random seed for eval sample selection [%(default)s]")

args = parser.parse_args()

# If neither --train nor --eval specified, do both
if not args.train and not args.eval:
    args.train = True
    args.eval  = True

# Parse held-out species
val_species_set = set()
if args.val_species:
    val_species_set = {s.strip() for s in args.val_species.split(",")}

n_workers_per_species = args.n_workers or max(1, multiprocessing.cpu_count() - 1)
raw_dir               = pathlib.Path(args.raw_dir)

# Auto-discover species from raw dir
all_discovered = util.discover_species(raw_dir)

if not all_discovered:
    print("No valid species found in raw directory!")
    sys.exit(1)

random.seed(args.seed)


# Measure genome size for load balancing
def _genome_size(name, path):
    fna = pathlib.Path(path) / "fna.gz"
    return fna.stat().st_size if fna.exists() else 0


# Split held-out vs training species
train_species   = [(name, path) for name, path in all_discovered if name not in val_species_set]
heldout_species = [(name, path) for name, path in all_discovered if name in val_species_set]

# Greedy bin-packing for training species only
train_species.sort(key=lambda sp: _genome_size(*sp), reverse=True)
bins      = [[] for _ in range(args.nnodes)]
bin_sizes = [0]  * args.nnodes

for sp in train_species:
    lightest          = bin_sizes.index(min(bin_sizes))
    bins[lightest].append(sp)
    bin_sizes[lightest] += _genome_size(*sp)

species_to_process = bins[args.node_rank]

# Node 0 also handles held-out species
if args.node_rank == 0:
    heldout_to_process = heldout_species
else:
    heldout_to_process = []

if not species_to_process and not heldout_to_process:
    print("No species to process!")
    sys.exit(0)

output_dir = pathlib.Path(args.output_dir)
log_dir    = pathlib.Path(args.log_dir)

output_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)


####################
#####  Header  #####
####################


modes = []
if args.train:
    modes.append("train")
if args.eval:
    modes.append("eval")

print(f"\n{'='*60}")
print(f"{'GeneT5 Data Baker':^60}")
print(f"{'='*60}")
print(f"  Raw directory:    {raw_dir}")
print(f"  Output directory: {output_dir}")
print(f"  Tokenizer:        {args.tokenizer} (fast)")
print(f"  Mode:             {' + '.join(modes)}")
print(f"")
print(f"  Window size:      {args.window_size:,} bp")
print(f"  Canonical only:   True")
if val_species_set:
    print(f"  Held-out species: {', '.join(sorted(val_species_set))}")
if args.eval:
    print(f"  Eval samples:     {args.eval_samples} per held-out species")
print(f"")
if args.nnodes > 1:
    total_size = sum(_genome_size(*s) for s in species_to_process)
    print(f"  Node:             {args.node_rank} of {args.nnodes}")
    print(f"  This node:        {len(species_to_process)} species ({total_size / 1024**3:.1f} GB)")
    print(f"  Total species:    {len(train_species)}")
else:
    print(f"  Species:          {len(species_to_process)}")
print(f"  Species parallel: {args.species_parallel}")
print(f"  Workers/species:  {n_workers_per_species}")

print(f"\n{' Species Summary ':=^60}")
print(f"  Discovered: {len(all_discovered)} species")
print(f"  Training:   {len(species_to_process)}")
print(f"  Held-out:   {len(heldout_to_process)}")
for name, _ in sorted(species_to_process):
    print(f"    {name}")


##############################
#####  Process Species   #####
##############################


def _make_jobs(species_list):
    """Build BakeJob list from (name, path) tuples."""
    return [
        util.BakeJob(
            species     = name,
            raw_dir     = str(raw_dir),
            output_dir  = str(output_dir),
            log_dir     = str(log_dir),
            window_size = args.window_size,
            tokenizer   = args.tokenizer,
            n_workers   = n_workers_per_species,
            compress    = args.compress,
        )
        for name, path in species_list
    ]


if args.train:
    work_items = _make_jobs(species_to_process)

    print(f"\n{' Processing Training Species ':=^60}")

    results = []
    success = 0
    failed  = 0

    with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
        futures = {executor.submit(util.process_species, job): job.species for job in work_items}

        for future in concurrent.futures.as_completed(futures):
            species_name = futures[future]
            try:
                result = future.result()
                results.append(result)

                if result["success"]:
                    success += 1
                    print(f"  + {species_name}")
                else:
                    failed += 1
                    error = result.get("error", "Unknown error")
                    print(f"  x {species_name}: {error}")

            except Exception as e:
                failed += 1
                results.append({"species": species_name, "success": False, "error": str(e)})
                print(f"  x {species_name}: {e}")

    print(f"\n{' Processing Results ':=^60}")
    print(f"  Success: {success}")
    print(f"  Failed:  {failed}")


#####################################
#####  Held-Out Species (Val)   #####
#####################################


if heldout_to_process and args.train:
    print(f"\n{' Held-Out Validation Species ':=^60}")

    heldout_items = _make_jobs(heldout_to_process)

    with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
        futures = {executor.submit(util.process_species, job): job.species for job in heldout_items}

        for future in concurrent.futures.as_completed(futures):
            species_name = futures[future]
            try:
                result = future.result()
                if result["success"]:
                    print(f"  + {species_name} (held-out)")
                else:
                    error = result.get("error", "Unknown error")
                    print(f"  x {species_name}: {error}")

            except Exception as e:
                print(f"  x {species_name}: {e}")


##############################
#####  Merge Outputs     #####
##############################


if args.train:
    import lib.dataset as ds

    print(f"\n{' Merging Per-Species Bins ':=^60}")

    # Training species -> training.bin, held-out species -> validation.bin
    train_species_names   = {name for name, _ in species_to_process}
    heldout_species_names = {name for name, _ in heldout_to_process} if heldout_to_process else set()

    training_bins = sorted(
        f for f in output_dir.glob("*/training.bin")
        if f.parent.name in train_species_names
    )
    validation_bins = sorted(
        f for f in output_dir.glob("*/training.bin")
        if f.parent.name in heldout_species_names
    )

    print(f"  Training files:   {len(training_bins)}")
    print(f"  Validation files: {len(validation_bins)} (held-out species)")

    for f in training_bins:
        size_mb = f.stat().st_size / 1024 / 1024
        print(f"    {f.parent.name:30s} {size_mb:.1f} MB")

    # Stream-merge all per-species bins into one
    if training_bins:
        print(f"\n  Training merge:")
        ds.merge_binary_files(
            training_bins,
            output_dir / "training.bin",
            compress      = args.compress,
            show_progress = True,
        )

    if validation_bins:
        print(f"\n  Validation merge (capped at {args.val_windows:,}):")
        ds.merge_binary_files(
            validation_bins,
            output_dir / "validation.bin",
            compress      = args.compress,
            show_progress = True,
            max_chunks    = args.val_windows,
        )


#####################################
#####  Generate Eval Samples    #####
#####################################


all_eval_samples = []

if heldout_to_process and args.eval:
    import lib.dataset as ds

    print(f"\n{' Generating Eval Samples ':=^60}")

    for sp, _ in heldout_to_process:
        species_dir = raw_dir / sp

        # Try loading gene_index sidecar first (avoids re-parsing GFF)
        sidecar_path = output_dir / sp / "gene_index.json"
        gene_index   = ds.load_gene_index(sidecar_path)

        if gene_index is not None:
            print(f"  {sp}: loaded gene_index from sidecar")
        else:
            print(f"  {sp}: WARNING - no sidecar, re-parsing GFF")
            fna_path, gff_path = ds.find_genome_files(species_dir)
            features   = ds.parse_gff(gff_path)
            gene_index = ds.build_gene_index(features)
            gene_index = ds.filter_canonical_transcripts(gene_index)

        # FASTA still needs to be read for eval (need actual sequences)
        fna_path, _ = ds.find_genome_files(species_dir)
        sequences   = ds.parse_fasta(fna_path)
        coding      = ds.extract_coding_genes(gene_index)

        species_samples = []
        for gene_id, gene_data in coding.items():
            sample = ds.build_eval_sample(gene_id, gene_data, sequences, args.window_size)
            if sample:
                sample["species"] = sp
                species_samples.append(sample)

        selected = ds.select_diverse_samples(species_samples, args.eval_samples)
        all_eval_samples.extend(selected)
        print(f"  {sp}: {len(selected)} eval samples from {len(species_samples)} candidates")

    # Write eval.json
    if all_eval_samples:
        eval_path = output_dir / "eval.json"
        with open(eval_path, 'w') as f:
            json.dump(all_eval_samples, f)

        total_exons = sum(s["num_exons"] for s in all_eval_samples)
        size_mb     = eval_path.stat().st_size / 1024 / 1024
        print(f"\n  eval.json: {len(all_eval_samples)} samples, {total_exons} exons ({size_mb:.1f} MB)")


##############################
#####  Bake Config       #####
##############################


bake_config = {
    **{k: str(v) if isinstance(v, pathlib.Path) else v for k, v in vars(args).items()},
    "train_species":   [name for name, _ in species_to_process],
    "heldout_species": [name for name, _ in heldout_to_process],
    "eval_samples_generated": len(all_eval_samples),
}

config_path = output_dir / "bake_config.json"
with open(config_path, 'w') as f:
    json.dump(bake_config, f, indent=2)


##############################
#####  Final Summary     #####
##############################


print(f"\n{' Final Output ':=^60}")

if args.train:
    import lib.dataset as ds

    merged_train = output_dir / "training.bin"
    merged_val   = output_dir / "validation.bin"

    if merged_train.exists():
        info = ds.get_binary_info(merged_train)
        size = ds.format_size(merged_train.stat().st_size)
        print(f"  training.bin:    {info['num_chunks']:,} chunks  ({size})")

    if merged_val.exists():
        info = ds.get_binary_info(merged_val)
        size = ds.format_size(merged_val.stat().st_size)
        print(f"  validation.bin:  {info['num_chunks']:,} chunks  ({size})")

if all_eval_samples:
    print(f"  eval.json:       {len(all_eval_samples)} samples")

print(f"  bake_config.json")

# Augmentation report
if args.train:
    util.report_augmentation_status(output_dir)

print(f"\n{'='*60}")
print("Done!")
print(f"{'='*60}")
