#!/usr/bin/env python3

import sys
import os
import argparse
import tempfile
import shutil
import multiprocessing
import pathlib
import subprocess
import concurrent.futures
import json

import lib.util as util


parser = argparse.ArgumentParser(
    description="Bake all species data: parse and tokenize")
parser.add_argument("--raw_dir",    type=str, default="../raw",
    help="Directory containing species subdirectories [%(default)s]")
parser.add_argument("--output_dir", type=str, default="../baked",
    help="Output directory for baked .bin files [%(default)s]")
parser.add_argument("--log_dir",    type=str, default="../logs/baker",
    help="Directory for log files [%(default)s]")
parser.add_argument("--taxa",       type=str, nargs='+', default=None,
    help="Process only specific taxa (default: all)")
parser.add_argument("--species",    type=str, nargs='+', default=None,
    help="Process only specific species (default: all)")
parser.add_argument("--window_size", type=int, default=20000,
    help="Window size in base pairs [%(default)s]")
parser.add_argument("--species_parallel", type=int, default=3,
    help="Number of species to process in parallel [%(default)s]")
parser.add_argument("--n_workers",  type=int, default=None,
    help="Workers per species for chunking [auto]")
parser.add_argument("--tokenizer",  type=str, required=True,
    help="Tokenizer path (required)")
parser.add_argument("--num_complex", type=int, default=10,
    help="Number of complex genes for validation per species [%(default)s]")
parser.add_argument("--num_normal",  type=int, default=10,
    help="Number of normal genes for validation per species [%(default)s]")
parser.add_argument("--num_easy",    type=int, default=10,
    help="Number of easy genes for validation per species [%(default)s]")
parser.add_argument("--compress", type=str, default=None,
    choices=['zlib', 'zstd'], help="Compress binary files with zlib or zstd")
parser.add_argument("--canonical_only", action="store_true",
    help="Keep only canonical (longest) transcript per gene")
parser.add_argument("--nnodes",    type=int, default=1,
    help="Total number of baking nodes [%(default)s]")
parser.add_argument("--node_rank", type=int, default=0,
    help="This node's rank (0-indexed) [%(default)s]")

args = parser.parse_args()

n_workers_per_species = args.n_workers or max(1, multiprocessing.cpu_count() - 1)
all_species           = util.build_species_list(args.species, args.taxa, args.window_size)
raw_dir               = pathlib.Path(args.raw_dir)

# Measure genome size for load balancing
def _genome_size(entry):
    fna = raw_dir / entry[0] / "fna.gz"
    return fna.stat().st_size if fna.exists() else 0

# Greedy bin-packing: assign each species to the lightest node
all_species.sort(key=_genome_size, reverse=True)
bins      = [[] for _ in range(args.nnodes)]
bin_sizes = [0]  * args.nnodes

for sp in all_species:
    lightest          = bin_sizes.index(min(bin_sizes))
    bins[lightest].append(sp)
    bin_sizes[lightest] += _genome_size(sp)

species_to_process = bins[args.node_rank]

if not species_to_process:
    print("No species to process!")
    sys.exit(0)

output_dir = pathlib.Path(args.output_dir)
log_dir    = pathlib.Path(args.log_dir)

output_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)


####################
#####  Header  #####
####################


print(f"\n{'='*60}")
print(f"{'GeneT5 Data Baker':^60}")
print(f"{'='*60}")
print(f"  Raw directory:    {raw_dir}")
print(f"  Output directory: {output_dir}")
print(f"  Tokenizer:        {args.tokenizer}")
print(f"")
print(f"  Window size:      {args.window_size:,} bp")
print(f"  Canonical only:   {args.canonical_only}")
print(f"")
if args.nnodes > 1:
    total_size = sum(_genome_size(s) for s in species_to_process)
    print(f"  Node:             {args.node_rank} of {args.nnodes}")
    print(f"  This node:        {len(species_to_process)} species ({total_size / 1024**3:.1f} GB)")
    print(f"  Total species:    {len(all_species)}")
else:
    print(f"  Species:          {len(species_to_process)}")
print(f"  Species parallel: {args.species_parallel}")
print(f"  Workers/species:  {n_workers_per_species}")

print(f"\n{' Taxa Summary ':=^60}")

taxa_counts = {}
for sp, limit, taxa in species_to_process:
    taxa_counts[taxa] = taxa_counts.get(taxa, 0) + 1

for taxa, count in taxa_counts.items():
    print(f"  {taxa:15s}: {count:2d} species")


##############################
#####  Process Species   #####
##############################


work_items = [
    (sp, raw_dir, output_dir, log_dir, limit, None, args.tokenizer,
     n_workers_per_species, args.num_complex, args.num_normal, args.num_easy,
     args.compress, args.canonical_only)
    for sp, limit, taxa in species_to_process
]

print(f"\n{' Processing Species ':=^60}")

results = []
success = 0
failed  = 0

with concurrent.futures.ProcessPoolExecutor(max_workers=args.species_parallel) as executor:
    futures = {executor.submit(util.process_species, item): item[0] for item in work_items}

    for future in concurrent.futures.as_completed(futures):
        species_name = futures[future]
        try:
            result = future.result()
            results.append(result)

            if result["success"]:
                success += 1
                print(f"  + {species_name}")
            else:
                failed += 1
                error = result.get("error", "Unknown error")
                print(f"  x {species_name}: {error}")

        except Exception as e:
            failed += 1
            results.append({"species": species_name, "success": False, "error": str(e)})
            print(f"  x {species_name}: {e}")

print(f"\n{' Processing Results ':=^60}")
print(f"  Success: {success}")
print(f"  Failed:  {failed}")


##############################
#####  Merge Outputs     #####
##############################


import lib.dataset as ds

print(f"\n{' Merging Per-Species Bins ':=^60}")

training_bins   = sorted(output_dir.glob("*/training.bin"))
validation_bins = sorted(output_dir.glob("*/validation.bin"))

print(f"  Training files:   {len(training_bins)}")
print(f"  Validation files: {len(validation_bins)}")

for f in training_bins:
    size_mb = f.stat().st_size / 1024 / 1024
    print(f"    {f.parent.name:30s} {size_mb:.1f} MB")

# Stream-merge all per-species bins into one
if training_bins:
    print(f"\n  Training merge:")
    ds.merge_binary_files(
        training_bins,
        output_dir / "training.bin",
        compress       = args.compress,
        show_progress  = True,
    )

if validation_bins:
    print(f"\n  Validation merge:")
    ds.merge_binary_files(
        validation_bins,
        output_dir / "validation.bin",
        compress       = args.compress,
        show_progress  = True,
    )

# Final summary
print(f"\n{' Final Output ':=^60}")

merged_train = output_dir / "training.bin"
merged_val   = output_dir / "validation.bin"

if merged_train.exists():
    info = ds.get_binary_info(merged_train)
    size = ds.format_size(merged_train.stat().st_size)
    print(f"  training.bin:    {info['num_chunks']:,} chunks  ({size})")

if merged_val.exists():
    info = ds.get_binary_info(merged_val)
    size = ds.format_size(merged_val.stat().st_size)
    print(f"  validation.bin:  {info['num_chunks']:,} chunks  ({size})")

print(f"\n{'='*60}")
print("Done!")
print(f"{'='*60}")
