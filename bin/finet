#!/usr/bin/env python3

import argparse
import json
import random
import time
import pathlib
import os

import torch
import torch.distributed as dist

import torch.utils.data as data_utils
import transformers as tf

from contextlib import nullcontext
from liger_kernel.transformers.cross_entropy import LigerCrossEntropyLoss

import lib.dataset      as ds
import lib.tokenizer    as tk
import lib.model        as mdl
import lib.util         as util
import lib.util.train   as train_util


DEFAULTS = {
    'batch_size':     8,
    'lr':             1e-4,
    'epochs':         4,
    'weight_decay':   0.1,
    'warmup_ratio':   0.03,
    'grad_accum':     64,
    'max_grad_norm':  1.0,
    'num_workers':    2,
}


def main():
    
    parser = argparse.ArgumentParser(description='Fine-tune GeneT5')
    parser.add_argument('train_bin', type=str, metavar='<train>',
        help='training binary file')
    parser.add_argument('val_bin', type=str, metavar='<val>',
        help='validation binary file')
    parser.add_argument('output_dir', type=str, metavar='<o>',
        help='output directory')
    parser.add_argument('model_path', type=str, metavar='<model>',
        help='pretrained model path')
    parser.add_argument('--checkpoint', required=False, type=str, default=None,
        metavar='<file>', help='resume from checkpoint')
    parser.add_argument('--epochs', required=False, type=int, default=DEFAULTS['epochs'],
        metavar='<int>', help='number of epochs [%(default)i]')
    parser.add_argument('--batch_size', required=False, type=int, default=DEFAULTS['batch_size'],
        metavar='<int>', help='batch size [%(default)i]')
    parser.add_argument('--lr', required=False, type=float, default=DEFAULTS['lr'],
        metavar='<float>', help='learning rate [%(default)g]')
    parser.add_argument('--grad_accum', required=False, type=int, default=DEFAULTS['grad_accum'],
        metavar='<int>', help='gradient accumulation steps [%(default)i]')
    parser.add_argument('--weight_decay', required=False, type=float, default=DEFAULTS['weight_decay'],
        metavar='<float>', help='weight decay [%(default).2f]')
    parser.add_argument('--warmup_ratio', required=False, type=float, default=DEFAULTS['warmup_ratio'],
        metavar='<float>', help='warmup ratio [%(default).2f]')
    parser.add_argument('--max_grad_norm', required=False, type=float, default=DEFAULTS['max_grad_norm'],
        metavar='<float>', help='max gradient norm [%(default).1f]')
    parser.add_argument('--seed', required=False, type=int, default=42,
        metavar='<int>', help='random seed [%(default)i]')
    parser.add_argument('--save_every', required=False, type=int, default=1,
        metavar='<int>', help='save checkpoint every N epochs [%(default)i]')
    parser.add_argument('--num_workers', required=False, type=int, default=DEFAULTS['num_workers'],
        metavar='<int>', help='dataloader workers [%(default)i]')
    parser.add_argument('--early_stopping', required=False, type=int, default=None,
        metavar='<int>', help='early stopping patience')
    parser.add_argument('--label_smoothing', required=False, type=float, default=0.0,
        metavar='<float>', help='label smoothing factor [%(default).2f]')
    parser.add_argument('--gradient_checkpointing', action='store_true',
        help='enable gradient checkpointing to save memory')
    parser.add_argument('--log_every_pct', required=False, type=int, default=10,
        metavar='<int>', help='log progress every N percent of epoch [%(default)i]')
    parser.add_argument('--save_steps', required=False, type=int, default=None,
        metavar='<int>', help='save checkpoint every N optimizer steps (default: end of epoch only)')
    parser.add_argument('--optim_8bit', action='store_true',
        help='use 8-bit Adam optimizer (requires bitsandbytes, saves ~50%% optimizer memory)')
    parser.add_argument('--empty_cache_steps', required=False, type=int, default=None,
        metavar='<int>', help='call torch.cuda.empty_cache() every N steps to reduce fragmentation')
    parser.add_argument('--no_pin_memory', action='store_true',
        help='disable pin_memory in dataloaders (can reduce CPU memory pressure)')
    parser.add_argument('--memwatch', action='store_true',
        help='enable background memory monitoring to CSV (first 5min: 5s intervals, then 30s)')
    parser.add_argument('--mxfp8', action='store_true',
        help='apply MXFP8 quantization to dense Linear layers (Blackwell only)')
    parser.add_argument('--compile', action='store_true',
        help='enable torch.compile on encoder and decoder')

    args = parser.parse_args()

    # distributed setup (auto-detected from torchrun env vars)
    dist_info = train_util.setup_distributed(backend="nccl")
    is_dist   = dist_info is not None
    is_main   = train_util.is_main_process()

    if is_main:
        print(f"\n{' GeneT5 Fine-Tuning ':=^60}")

    if is_dist and is_main:
        print(f"Distributed: {dist_info['world_size']} nodes, rank {dist_info['rank']}")

    # device setup
    device = train_util.get_device()
    if is_main:
        print(f"Device: {device}")

    # set seeds
    torch.manual_seed(args.seed)
    random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
        torch.set_float32_matmul_precision('high')
    
    output_dir = pathlib.Path(args.output_dir)
    if is_main:
        output_dir.mkdir(parents=True, exist_ok=True)
    if is_dist:
        train_util.barrier()

    # load tokenizer
    if is_main:
        print(f"\nLoading tokenizer...")
    tokenizer = tk.GeneTokenizer(pathlib.Path(args.model_path))
    if is_main:
        print(f"  Vocab size: {len(tokenizer)}")

    # load model
    if is_main:
        print(f"\nLoading model...")
    model = mdl.GeneT5.from_pretrained(pathlib.Path(args.model_path), device='cpu', dtype=torch.bfloat16)

    # Regional torch.compile (before DDP so DDPOptimizer sees compiled submodules)
    if args.compile:
        if is_main:
            print("\nCompiling model regions...")
        compile_opts = {"mode": "default", "dynamic": None}
        model.encoder = torch.compile(model.encoder, **compile_opts)
        model.decoder = torch.compile(model.decoder, **compile_opts)
        if is_main:
            print("  Compiled: encoder, decoder")
            print("  First ~3 batches will be slow (compilation warmup)")

    model = train_util.wrap_model_distributed(model, device,
        find_unused_params=False, static_graph=False)

    # Halve allreduce volume by compressing gradients to BF16
    if is_dist:
        from torch.distributed.algorithms.ddp_comm_hooks import default_hooks
        model.register_comm_hook(state=None, hook=default_hooks.bf16_compress_hook)

    raw_model = train_util.unwrap_model(model)
    stats     = raw_model.get_param_stats()
    if is_main:
        print(f"  Trainable: {stats['total_trainable']:,}")
        print(f"  Frozen:    {stats['total_frozen']:,}")

    # apply MXFP8 quantization to dense Linear layers
    if args.mxfp8:
        from torchao.prototype.mx_formats import MXLinearConfig
        from torchao.quantization import quantize_

        mx_config = MXLinearConfig.from_recipe_name("mxfp8_cublas")

        def mxfp8_filter(mod, fqn):
            if not isinstance(mod, torch.nn.Linear):
                return False
            if mod.in_features % 32 != 0 or mod.out_features % 32 != 0:
                return False
            if 'cross_attn.q' in fqn or 'cross_attn.o' in fqn:
                return False
            return True

        quantize_(raw_model, mx_config, filter_fn=mxfp8_filter)
        n_mx = sum(1 for _, m in raw_model.named_modules() if type(m).__name__ == 'MXLinear')
        if is_main:
            print(f"  MXFP8: converted {n_mx} Linear layers to MXLinear")

    # load datasets
    if is_main:
        print(f"\nLoading datasets...")

    train_dataset = ds.BinaryTrainDataset(args.train_bin, tokenizer, args.seed)
    val_dataset   = ds.BinaryTrainDataset(args.val_bin, tokenizer, args.seed)

    if is_main:
        print(f"  Train: {len(train_dataset):,}")
        print(f"  Val:   {len(val_dataset):,}")

    if len(train_dataset) == 0:
        if is_main:
            print(f"\nERROR: Training dataset is empty!")
            print(f"       Please check: {args.train_bin}")
        if is_dist:
            train_util.cleanup_distributed()
        return

    # setup dataloaders
    if is_main:
        print(f"\nSetting up dataloaders...")

    collator = ds.DynamicPaddingCollator(tokenizer.pad_token_id, -100)

    # Distributed sampler for multi-node training
    dist_sampler     = None
    val_dist_sampler = None
    if is_dist:
        dist_sampler     = data_utils.distributed.DistributedSampler(train_dataset, shuffle=True)
        val_dist_sampler = data_utils.distributed.DistributedSampler(val_dataset, shuffle=False)

    persistent = args.num_workers > 0

    train_loader = data_utils.DataLoader(
        train_dataset,
        batch_size         = args.batch_size,
        shuffle            = not is_dist,
        sampler            = dist_sampler,
        collate_fn         = collator,
        num_workers        = args.num_workers,
        pin_memory         = not args.no_pin_memory,
        drop_last          = True,
        persistent_workers = persistent,
    )

    val_loader = data_utils.DataLoader(
        val_dataset,
        batch_size         = args.batch_size,
        shuffle            = False,
        sampler            = val_dist_sampler,
        collate_fn         = collator,
        num_workers        = args.num_workers,
        pin_memory         = not args.no_pin_memory,
        persistent_workers = persistent,
    )
    
    if is_main:
        print(f"  Train batches: {len(train_loader)}")
        print(f"  Val batches:   {len(val_loader)}")

    # setup optimizer
    if is_main:
        print(f"\nSetting up optimizer...")

    # Scale learning rate for distributed training (linear scaling rule)
    effective_lr = args.lr * train_util.get_world_size() if is_dist else args.lr

    if args.optim_8bit:
        try:
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit(
                model.parameters(),
                lr           = effective_lr,
                betas        = (0.9, 0.95),
                weight_decay = args.weight_decay,
            )
            if is_main:
                print("  Using 8-bit AdamW (saves ~12GB optimizer memory)")
        except (ImportError, RuntimeError) as e:
            if is_main:
                print(f"  WARNING: bitsandbytes not available ({type(e).__name__}), falling back to standard AdamW")
                print("           Install/fix with: pip install bitsandbytes")
            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr           = effective_lr,
                betas        = (0.9, 0.95),
                weight_decay = args.weight_decay,
                fused        = True,
            )
    else:
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr           = effective_lr,
            betas        = (0.9, 0.95),
            weight_decay = args.weight_decay,
            fused        = True,
        )
    
    total_steps  = len(train_loader) * args.epochs // args.grad_accum
    warmup_steps = int(total_steps * args.warmup_ratio)
    
    scheduler = tf.get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)
    
    if is_main:
        if is_dist:
            print(f"  Base LR:      {args.lr}")
            print(f"  Scaled LR:    {effective_lr} (x{train_util.get_world_size()} nodes)")
        print(f"  Total steps:  {total_steps}")
        print(f"  Warmup steps: {warmup_steps}")
    
    # handle checkpoint
    start_epoch   = 0
    best_val_loss = float('inf')
    
    if args.checkpoint:
        checkpoint = torch.load(args.checkpoint, map_location=device)

        load_target = train_util.unwrap_model(model)
        load_target.load_state_dict(checkpoint['model_state_dict'])

        if 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        start_epoch   = checkpoint.get('epoch', 0)
        best_val_loss = checkpoint.get('config', {}).get('best_val_loss', float('inf'))

        if is_main:
            print(f"  Loaded checkpoint from {args.checkpoint} (epoch {start_epoch})")
    
    # save config
    if is_main:
        config = {
            **vars(args),
            'vocab_size':    len(tokenizer),
            'train_samples': len(train_dataset),
            'val_samples':   len(val_dataset),
            'world_size':    train_util.get_world_size(),
            'effective_lr':  effective_lr,
        }

        with open(output_dir / 'finetune_config.json', 'w') as f:
            json.dump(config, f, indent=2)

    # training loop
    if is_main:
        print(f"\n{'=' * 60}")
        print('Training...')
        print(f"{'=' * 60}")

    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

    # Setup label smoothing loss if enabled (use Liger fused kernel for memory efficiency)
    if args.label_smoothing > 0:
        loss_fct = LigerCrossEntropyLoss(ignore_index=-100, label_smoothing=args.label_smoothing)
        if is_main:
            print(f"  Label smoothing: {args.label_smoothing}")
    else:
        loss_fct = None

    # Enable gradient checkpointing if requested
    if args.gradient_checkpointing:
        if hasattr(raw_model.encoder, 'gradient_checkpointing_enable'):
            raw_model.encoder.gradient_checkpointing_enable()
        if hasattr(raw_model.decoder, 'gradient_checkpointing_enable'):
            raw_model.decoder.gradient_checkpointing_enable()
        if is_main:
            print("  Gradient checkpointing: enabled")

    # Print progress logging info
    batches_per_epoch = len(train_loader)
    optimizer_steps_per_epoch = batches_per_epoch // args.grad_accum
    if is_main:
        print(f"  Progress logging: every {args.log_every_pct}% ({batches_per_epoch * args.log_every_pct // 100} batches)")
        if args.save_steps:
            print(f"  Step checkpoints: every {args.save_steps} optimizer steps")
        else:
            print(f"  Step checkpoints: disabled (use --save_steps N to enable)")

    if args.compile and is_main:
        print("  torch.compile warmup: first 3-5 batches will be 10-60x slower")

    patience_counter = 0
    global_step = 0  # Track optimizer steps across epochs

    # Start memory watcher if enabled (rank 0 only, output dir may be read-only on workers)
    mem_watcher = None
    if args.memwatch and is_main:
        mem_watcher = util.create_memory_watcher(output_dir, prefix="memory")
        mem_watcher.start()

    # Training metrics logger (CSV + stdout, rank 0 only)
    train_logger = None
    if is_main:
        train_logger = util.create_train_logger(output_dir)
        print(f"  Training log: {train_logger.log_path}")

    for epoch in range(start_epoch, args.epochs):
        if is_main:
            print(f"\nEpoch {epoch + 1}/{args.epochs}")
            print('-' * 40)

        # Sync distributed sampler epoch for proper shuffling
        if dist_sampler:
            dist_sampler.set_epoch(epoch)
        if val_dist_sampler:
            val_dist_sampler.set_epoch(epoch)
        train_dataset.set_epoch(epoch)

        # train epoch
        model.train()
        total_train_loss = 0

        optimizer.zero_grad(set_to_none=True)

        # Progress tracking
        num_batches = len(train_loader)
        log_interval = max(1, num_batches * args.log_every_pct // 100)
        last_logged_pct = -1
        epoch_start_time = time.time()

        for step, batch in enumerate(train_loader):
            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}

            # Prepare decoder inputs with proper attention mask
            decoder_input_ids = batch['labels'][:, :-1].clone()
            labels = batch['labels'][:, 1:]

            # Create decoder attention mask (1 for valid tokens, 0 for padding)
            decoder_attention_mask = (decoder_input_ids != -100).long()

            # Replace -100 with pad_token_id (can't pass -100 to embedding layer)
            decoder_input_ids[decoder_input_ids == -100] = tokenizer.pad_token_id

            # Only sync gradients on accumulation boundary
            is_sync_step = (step + 1) % args.grad_accum == 0
            sync_context = nullcontext() if (not is_dist or is_sync_step) else model.no_sync()

            with sync_context:
                with torch.amp.autocast('cuda', dtype=dtype):
                    outputs = model(
                        encoder_input_ids      = batch['input_ids'],
                        decoder_input_ids      = decoder_input_ids,
                        labels                 = labels if loss_fct is None else None,
                        decoder_attention_mask = decoder_attention_mask,
                    )

                    # Use label smoothing loss if enabled
                    if loss_fct is not None:
                        logits = outputs['logits']
                        loss = loss_fct(logits.reshape(-1, raw_model.vocab_size), labels.reshape(-1))
                        if outputs.get('moe_loss') is not None:
                            loss = loss + outputs['moe_loss']
                        del logits
                    else:
                        loss = outputs['loss']

                    loss = loss / args.grad_accum

                # Save loss value before backward (for logging)
                loss_val = loss.item() * args.grad_accum

                loss.backward()

            del outputs, loss, batch, decoder_input_ids, labels, decoder_attention_mask

            if (step + 1) % args.grad_accum == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad(set_to_none=True)
                global_step += 1

                # Step-based checkpoint saving (rank 0 only)
                if args.save_steps and global_step % args.save_steps == 0 and is_main:
                    save_path = output_dir / f'checkpoint_step_{global_step}.pt'
                    step_model = train_util.unwrap_model(model)
                    checkpoint = {
                        'epoch':                epoch,
                        'global_step':          global_step,
                        'model_state_dict':     step_model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'config':               {'best_val_loss': best_val_loss},
                    }
                    torch.save(checkpoint, save_path)
                    print(f"  [Step {global_step}] Saved checkpoint: {save_path.name}")

                # Periodic cache clearing to reduce memory fragmentation
                if args.empty_cache_steps and global_step % args.empty_cache_steps == 0:
                    torch.cuda.empty_cache()

            total_train_loss += loss_val

            # Progress logging every N% (rank 0 only)
            current_pct = (step + 1) * 100 // num_batches
            if train_logger and current_pct >= last_logged_pct + args.log_every_pct:
                last_logged_pct = (current_pct // args.log_every_pct) * args.log_every_pct
                elapsed         = time.time() - epoch_start_time
                batch_per_sec   = (step + 1) / elapsed if elapsed > 0 else 0
                avg_loss        = total_train_loss / (step + 1)

                train_logger.log_step(
                    epoch         = epoch + 1,
                    global_step   = global_step,
                    batch         = step + 1,
                    num_batches   = num_batches,
                    loss          = avg_loss,
                    lr            = scheduler.get_last_lr()[0],
                    batch_per_sec = batch_per_sec,
                )

        train_loss = total_train_loss / max(len(train_loader), 1)

        # Sync train loss across processes
        if is_dist:
            loss_tensor = torch.tensor([train_loss], device=device)
            train_util.all_reduce_mean(loss_tensor)
            train_loss = loss_tensor.item()

        # evaluate
        model.eval()
        total_val_loss = 0
        num_batches    = 0

        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}

                # Prepare decoder inputs with proper attention mask
                decoder_input_ids = batch['labels'][:, :-1].clone()
                labels = batch['labels'][:, 1:]
                decoder_attention_mask = (decoder_input_ids != -100).long()
                decoder_input_ids[decoder_input_ids == -100] = tokenizer.pad_token_id

                with torch.amp.autocast('cuda', dtype=dtype):
                    outputs = model(
                        encoder_input_ids      = batch['input_ids'],
                        decoder_input_ids      = decoder_input_ids,
                        labels                 = labels,
                        decoder_attention_mask = decoder_attention_mask,
                    )

                total_val_loss += outputs['loss'].item()
                num_batches    += 1

                del outputs, batch, decoder_input_ids, labels, decoder_attention_mask

        val_loss = total_val_loss / max(num_batches, 1)

        # Sync val loss across processes
        if is_dist:
            val_tensor = torch.tensor([val_loss], device=device)
            train_util.all_reduce_mean(val_tensor)
            val_loss = val_tensor.item()

        if train_logger:
            train_logger.log_epoch(epoch + 1, train_loss, val_loss, scheduler.get_last_lr()[0])

        # checkpointing (rank 0 saves, all ranks wait)
        save_model = train_util.unwrap_model(model)

        if val_loss < best_val_loss:
            best_val_loss    = val_loss
            patience_counter = 0

            if is_main:
                print(f"  New best! Saving best_model.pt")

                save_path = output_dir / 'best_model.pt'
                save_path.parent.mkdir(parents=True, exist_ok=True)

                checkpoint = {
                    'epoch':                epoch + 1,
                    'model_state_dict':     save_model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'config':               {'best_val_loss': best_val_loss, 'best_epoch': epoch + 1},
                }
                torch.save(checkpoint, save_path)

            train_util.barrier()
        else:
            patience_counter += 1
            es_str            = args.early_stopping if args.early_stopping else 'inf'
            if is_main:
                print(f"  No improvement ({patience_counter}/{es_str})")

            if args.early_stopping and patience_counter >= args.early_stopping:
                if is_main:
                    print(f"\n  Early stopping!")
                break

        if (epoch + 1) % args.save_every == 0 and is_main:
            save_path = output_dir / 'checkpoint_latest.pt'
            save_path.parent.mkdir(parents=True, exist_ok=True)

            checkpoint = {
                'epoch':                epoch + 1,
                'model_state_dict':     save_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'config':               {'best_val_loss': best_val_loss},
            }
            torch.save(checkpoint, save_path)

        train_util.barrier()
    
    # Stop memory watcher and close logger
    if mem_watcher:
        mem_watcher.stop()
    if train_logger:
        train_logger.close()

    # save final (rank 0 only)
    if is_main:
        print(f"\n{'=' * 60}")
        print('Saving final model...')

        final_model = train_util.unwrap_model(model)
        final_model.save(output_dir / 'pytorch_model.bin')
        tokenizer.save_pretrained(output_dir)

        model_path = pathlib.Path(args.model_path)
        with open(model_path / 'config.json') as f:
            model_config = json.load(f)
        with open(output_dir / 'config.json', 'w') as f:
            json.dump(model_config, f, indent=2)

        print(f"\n{'=' * 60}")
        print(f"Complete!")
        print(f"  Output: {output_dir}")
        print(f"  Best Val Loss: {best_val_loss:.4f}")
        print(f"{'=' * 60}")

    # cleanup distributed
    if is_dist:
        train_util.cleanup_distributed()


if __name__ == '__main__':
    main()