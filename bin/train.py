"""Simple training script for the IntraNet CNN+BLSTM model."""

import argparse
import math
from pathlib import Path

import torch
from torch import nn
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, random_split

from lib import intranet as tk


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train the IntraNet CNN+BLSTM model on tokenised genomic data.",
    )
    parser.add_argument(
        "--corpus",
        required=True,
        type=str,
        help="Path to the tokenised corpus generated by data2token.py.",
    )
    parser.add_argument(
        "--labels",
        required=True,
        type=str,
        help="Path to the label sequence file aligned with the corpus.",
    )
    parser.add_argument(
        "--embeddings",
        required=True,
        type=str,
        help="Path to the pre-trained embedding matrix (GloVe format).",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=10,
        help="Number of training epochs [%(default)d].",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="Mini-batch size [%(default)d].",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-3,
        help="Learning rate for Adam optimiser [%(default)g].",
    )
    parser.add_argument(
        "--test-split",
        type=float,
        default=0.2,
        help="Fraction of data reserved for evaluation [%(default).2f].",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=0,
        help="Number of worker processes for data loading [%(default)d].",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for dataset splitting [%(default)d].",
    )
    parser.add_argument(
        "--save-path",
        type=str,
        default=None,
        help="Optional destination to store the trained model weights.",
    )
    parser.add_argument(
        "--use-cuda",
        action="store_true",
        help="Force training on CUDA if available.",
    )
    return parser.parse_args()


def collate_batch(batch):
    sequences, labels, lengths = zip(*batch)
    padded = pad_sequence(sequences, batch_first=True)
    labels = torch.tensor(labels, dtype=torch.long)
    lengths = torch.tensor(lengths, dtype=torch.long)
    return padded, labels, lengths

def main():
    args = parse_args()

    device = torch.device("cuda" if args.use_cuda and torch.cuda.is_available() else "cpu")
    if args.use_cuda and device.type != "cuda":
        print("Warning: CUDA requested but not available. Falling back to CPU.")

    embeddings = tk.parse_glove_matrix(args.embeddings)
    if not embeddings:
        raise ValueError(f"No embeddings parsed from {args.embeddings}.")

    sample_vector = next(iter(embeddings.values()))
    embedding_dim = sample_vector.shape[-1]

    dataset = tk.SplicingDataset(args.corpus, embeddings, args.labels)

    unique_labels = sorted(set(dataset.labels))
    num_classes = len(unique_labels)
    if num_classes < 2:
        raise ValueError(
            "At least two classes are required for training. "
            "Verify that the label file encodes multiple feature types."
        )

    train_loader, test_loader = prepare_dataloaders(
        dataset,
        batch_size=args.batch_size,
        test_split=args.test_split,
        num_workers=args.num_workers,
        seed=args.seed,
    )

    model = tk.IntraNet(embedding_dim=embedding_dim, num_classes=num_classes)
    model.to(device)

    loss_fn = nn.CrossEntropyLoss()
    optimiser = torch.optim.Adam(model.parameters(), lr=args.lr)

    for epoch in range(1, args.epochs + 1):
        model.train()
        running_loss = 0.0
        running_examples = 0

        for inputs, targets, lengths in train_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            lengths = lengths.to(device)

            optimiser.zero_grad()
            logits = model(inputs, lengths)
            loss = loss_fn(logits, targets)
            loss.backward()
            optimiser.step()

            running_loss += loss.item() * targets.size(0)
            running_examples += targets.size(0)

        train_loss = running_loss / running_examples if running_examples else float("nan")
        test_loss, test_accuracy = evaluate(model, test_loader, device, loss_fn)

        log_message = f"Epoch {epoch:03d}: train_loss={train_loss:.4f}"
        if not math.isnan(test_loss):
            log_message += f", test_loss={test_loss:.4f}, test_acc={test_accuracy:.4f}"
        print(log_message)

    if args.save_path:
        save_path = Path(args.save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(model.state_dict(), save_path)
        print(f"Model checkpoint saved to {save_path}")


if __name__ == "__main__":
    main()